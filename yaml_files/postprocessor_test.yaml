# Test YAML file for running the 'postprocessor' scanner plugin, for testing
# out point reweighting
#
# To test this out, first run:
# ./gambit -f largetest_diver+hdf5.yaml
# to generate output to be reweighted.
# Then run this file.
#
# Ben Farmer <benjamin.farmer@fysik.su.se>
# Feb 2015
#
 
#======== Input parameter declarations =======
#Parameters: 
#  # Prior type is "none" because postprocessor 'scanner' will read in old points
#  NormalDist:
#    mu:
#      prior_type: none
#    sigma:
#      prior_type: none

#======== Prior setup ========
#Priors:
#  read_from_file_prior:
#    parameters: [NormalDist::mu, NormalDist::sigma]
#    prior_type: plugin
#    plugin: parameter_reader


#======== Output setup ========
Printer:
  #printer: ascii
  #options:
  #  output_file: "results.txt"
  #  buffer_length: 1
  # Output can be re-printed into a different format via the postprocessor
  printer: hdf5
  options:
    output_file: "results.hdf5"
    group: "/reweight_data"
    delete_file_on_restart: true
    # Current a special setting is required here to allow the postproccessing scanner to print to points in an erratic order.
    #postproccess_mode: true
 
#======== Scanner setup ========
Scanner:
  use_scanner: postprocess

  scanners:

    postprocess:
      plugin: postprocessor

      # 'purpose' name assigned to log-likelihood for this 'scan'
      #like: LogLike3
      like: None
 
      # label given to result of likelihood reweighting. Can overwrite old name, if 'permit_discard_old_likes' is 'true'.
      reweighted_like: reweighted_LogLike2

      # If the purpose name above collides with an entry in the selected input file 
      # (e.g. an old likelihood from the original scan), should the old data be discarded?
      permit_discard_old_like: true

      # List of labels in old output to treat as likelihoods to be added to the newly computed LogLike.
      # It is assumed that these are stored as doubles. If they aren't, something unexpected may happen...
      add_to_like: [LogLike, LogLike]

      # List of labels in old output to treat as likelihoods to be subtracted from the newly computed LogLike.
      # (for example if you recompute a new likelihood component with a different tool, you may want to
      #  subtract the old component here)
      subtract_from_like: [LogLike, LogLike]
      # Note, there is no protection from adding then subtracting the same component, or from adding the same component twice etc.
      # More complicated arrangements of old + new likelihood components go beyond the goals of this scanner.
      # Use plotting tools for that kind of thing.
   
      # Renaming map for datasets
      # Use this to preserve input data that would otherwise be overwritten, or if you just
      # want to change the dataset name.
      # NOTE: There is no way to enforce that you stick to GAMBIT naming conventions here, but if don't
      # do so then you may be unable to postprocesses the new output, or it might screw with plotting tools.
      # So choose names wisely.
      # Also, don't try to rename e.g. the "is_valid" datasets in a HDF5 file. That will probably just crash,
      # since the reader will be unable to find the match "is_valid_is_valid" dataset ;)
      # Format is {"oldname1":"newname1", "oldname2":"newname2", ...}
      rename:  {"#normaldist_loglike @ExampleBit_A::lnL_gaussian": "old_normaldist_loglike",
                "LogLike" : "LogLike_old"}
                #"Runtime(ms) totalloop": "old_totalloop_runtime",
      # error example; clashes with 'LogLike' target;  "pointID": "LogLike2"}

      # Cut options
      # Use these to ignore certain entries in the input file.
      # Points for which all the cuts are TRUE will be postprocessed, points for which
      # any cut is FALSE will be ignored (but will still be copied into the new output, unless
      # the further option 'discard_points_outside_cuts: true' is set)
      # NOTE: These cuts currently only work for datasets with type 'double'.
      # Cuts can only be applied to OLD/INPUT datasets, not on newly computed observables.
      # For 'less than', the condition should be read as "variable_name" <= value. Similar for greater than.
      # That is, the cuts are e.g. 'less than or equal to'.
      # If a valid entry cannot be retrieved for one of these variables for a point, that point will
      # not be postprocessed. I.e. cuts are "failed" by default.
      cut_less_than:    {"#NormalDist_parameters @NormalDist::primary_parameters::mu": 22}
      cut_greater_than: {"#NormalDist_parameters @NormalDist::primary_parameters::mu": 21.8}
      discard_points_outside_cuts: true

      # Postprocess points as if we are in an MPI process with rank=RANK and size=SIZE (set via environment variable)
      # This lets a postprocessing batch be done completely in parallel
      # Note that output files will be appended with RANK, and will need to be manually combined at the end.
      # use_virtual_rank: true 

      # Set interval between progress report messages to stdout (0 for no messages, default 1000)
      update_interval: 100
      reader:
        # This determines what old output file is to be read and whose parameter points are to be used
        type: hdf5
        #type: ascii
        # Currently the asciiReader only works on one file, so they need to be combined before postprocessing. Just cat-ing them together should work.
        #info_filename: "runs/largetest_multinest+ascii/samples/results.txt_info_0"
        #data_filename: "runs/largetest_multinest+ascii/samples/results.txt_0"
        #file: "runs/spartan_multinest_hdf5/samples/results.hdf5"
        #file: "tmp.hdf5"
        #group: "/" 
        file: "runs/largetest_diver+hdf5/samples/results.hdf5"
        group: "/gambit_data" 
        #file: "runs/largetest_multinest+hdf5/samples/results.hdf5"
        #group: "/gambit_data3" 
        

#  objectives:
#
#    parameter_reader:
#      plugin: reweight_prior
 
                   
#======== Observables of interest ========
#ObsLikes:
#  - purpose:      LogLike3
#    capability:   normaldist_loglike
#    module:       ExampleBit_A
#    type:         double

#======== Rule entries ========
Rules:
  # None required

#======== Logging setup ========
Logger:
  redirection:
    [Default]      : "default.log"
    [Utilities]    : "utils.log"
    [Utilities,Info] : "utils_info.log"
    [ExampleBit_A] : "ExampleBit_A.log"
    [Scanner]      : "Scanner.log"
    [Printers]     : "Printers.log"

#======== Generic Name/Value Section ========
KeyValues:

  default_output_path: "runs/postprocessor_test/"

  print_timing_data: false

  rng: ranlux48

  likelihood:
    model_invalid_for_lnlike_below: -1e6

  debug: false
