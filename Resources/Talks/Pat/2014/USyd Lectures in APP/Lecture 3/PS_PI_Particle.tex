\documentclass[xcolor=dvipsnames]{beamer}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[absolute, overlay]{textpos}
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{soul}
\usepackage{wasysym}
\def\urltilda{\kern -.15em\lower .7ex\hbox{\~{}}\kern .04em}
\def\deg{^{\circ}}

\setlength{\TPHorizModule}{0.01\textwidth}
\setlength{\TPVertModule}{\TPHorizModule}
\definecolor{darkyellow}{rgb}{1,0.75,0}
\definecolor{black}{rgb}{0,0,0}
\definecolor{skyblue}{rgb}{0.7,0.8,1.0}
\definecolor{black}{rgb}{0,0,0}
\definecolor{darkgrey}{rgb}{0.3,0.3,0.3}
\definecolor{medgrey}{rgb}{0.5,0.5,0.5}
\definecolor{lightgrey}{rgb}{0.8,0.8,0.8}
\definecolor{lightMahogany}{rgb}{0.9,0.8,0.8}
\definecolor{darkblue}{rgb}{0,0,0.5}
\definecolor{darkgreen}{rgb}{0.1,0,0.3}
\definecolor{darkred}{rgb}{0.6,0,0}

\newcommand{\params}{\Xi}
\newcommand{\Eobsi}{E'_i}
\newcommand{\phiobsi}{\phi'_i}
\newcommand{\Etruei}{E_i}
\newcommand{\phitruei}{\phi_{i}}
\newcommand{\Eobsij}{E'_{ij}}
\newcommand{\phiobsij}{\phi'_{ij}}
\newcommand{\Etrueij}{E_{ij}}
\newcommand{\phitrueij}{\phi_{ij}}
\newcommand{\obs}{\mathrm{obs}}
\newcommand{\true}{\mathrm{true}}
\newcommand{\Like}{\mathcal{L}}
\newcommand{\ntot}{{n_\mathrm{tot}}}
\newcommand{\ntotj}{{n_{\mathrm{tot},j}}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\cblue}[1]{{\color[rgb]{0.1, 0.0, 0.6} #1}}
\newcommand{\cgreen}[1]{{\color[rgb]{0.0, 0.6, 0.1} #1}}
\newcommand{\corange}[1]{{\color[rgb]{0.9, 0.5, 0.0} #1}}
\newcommand{\cbluewhen}[2]{{\color#2[rgb]{0.1, 0.0, 0.6} #1}}
\newcommand{\cgreenwhen}[2]{{\color#2[rgb]{0.0, 0.6, 0.1} #1}}
\newcommand{\corangewhen}[2]{{\color#2[rgb]{0.9, 0.5, 0.0} #1}}

\newcommand{\manual}[1]{
  \begin{tikzpicture}[x=\textwidth, y=0.82\textheight]%, >=angle 90]
    \useasboundingbox (0, 0) rectangle (1, 1);
    #1
  \end{tikzpicture}
}

%%% Custom nodes.

\newcommand{\basenode}[4]{
  \node[outer sep=6pt, anchor=#3] (#1) at (#2){#4};
}

\newcommand{\emptynode}[2]{
  \basenode{#1}{#2}{base}{}

}

\newcommand{\minipagenode}[5]{
  \basenode{#1}{#2}{#3}{
    \begin{minipage}{#4}
      \vspace*{-12pt}
      {#5}
      \vspace*{-8pt}
  \end{minipage}}

}

\newcommand{\imagenode}[5]{
  \minipagenode{#1}{#2}{#3}{#4}{%
    \includegraphics[width=\textwidth]{#5}%
  }
}

%%% Connectors.

\newcommand{\connect}[4]{%
  \draw[<->, color=darkpurple, line width=1pt, out=#3, in=#4] (#1) to (#2);
}
\newcommand{\connectcolor}[5]{%
  \draw[<->, color=#5, line width=1pt, out=#3, in=#4] (#1) to (#2);
}
\newcommand{\hconnect}[3]{%
  \draw[<->, color=darkpurple, line width=1pt]
  (#1.east) .. controls +(right:#3) and +(left:#3) .. (#2.west);
}
\newcommand{\vconnect}[3]{%
  \draw[<->, color=darkpurple, line width=1pt]
  (#1.south) .. controls +(down:#3) and +(up:#3) .. (#2.north);
}

\newenvironment{litemize}
{\usebeamercolor[fg]{item color}%
\scriptsize\begin{list}{$\bullet$}{%
\setlength{\itemindent}{0pt}%
\setlength{\labelwidth}{10pt}%
\setlength{\leftmargin}{15pt}%
}}
{\end{list}}

\mode<presentation>
{
  \usetheme{Warsaw}
  \usecolortheme[named=Mahogany]{structure}
  \setbeamercovered{transparent}
  \setbeamercolor*{section in toc}{bg=white, fg=Mahogany}
}

%\beamerdefaultoverlayspecification{<+->}

\AtBeginSubsection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline}
  \begin{columns}[t]
\column{0.8\textwidth}
\tableofcontents[sections={1}, currentsection, currentsubsection]\vspace{3mm}

\tableofcontents[sections={2}, currentsection, currentsubsection]\vspace{3mm}

\tableofcontents[sections={3}, currentsection, currentsubsection]
  \end{columns}
  \end{frame}
}

\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline}
  \begin{columns}[t]
\column{0.8\textwidth}
\tableofcontents[sections={1}, currentsection, currentsubsection]\vspace{3mm}

\tableofcontents[sections={2}, currentsection, currentsubsection]\vspace{3mm}

\tableofcontents[sections={3}, currentsection, currentsubsection]
  \end{columns}
  \end{frame}
}

\setbeamertemplate{subsection in head/foot shaded}
{\textcolor{structure!70!white}{\insertsubsectionhead}}
\setbeamertemplate{subsection in head/foot}{\textcolor{white}\insertsubsectionhead}

\title[{\textcolor{white}Including astroparticle observables in BSM global fits}]{\textcolor{white}{Including astroparticle observables in global fits to new physics scenarios}}
\author[\textcolor{medgrey}{Pat Scott -- Oct 15 -- PI Particle Seminar}]{Pat Scott}
\institute{\small{Department of Physics, McGill University}}
\date[Feb 27 2013]{Slides available from \color[rgb]{0.1, 0.2, 0.6} \href{http://www.physics.mcgill.ca/~patscott}{\tt http://www.physics.mcgill.ca/{\urltilda}patscott}}
\pgfdeclareimage[height=0.7cm]{university-logo}{McGill_crest}
\logo{\pgfuseimage{university-logo}}

\subject{Talks}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}<beamer>
  \frametitle{Outline}
\begin{columns}[t]
  \column{0.8\textwidth}
  \tableofcontents[sections={1}]\vspace{3mm}

  \tableofcontents[sections={2}]\vspace{3mm}

  \tableofcontents[sections={3}]
\end{columns}
\end{frame}

\section{The Problem}

\begin{frame}
\frametitle{Searching for new physics}

\cblue{Many reasons to look for physics Beyond the Standard Model (BSM):}
\begin{itemize}
\item Higgs mass (hierarchy problem $+$ vacuum stability)
\item Dark matter exists
\item Baryon asymmetry
\item Neutrino masses and mixings
\end{itemize}

\visible<2->{
\cblue{So what do we do about it?}
\begin{itemize}
\item \cgreenwhen{Make new particles at high-$E$ colliders}{<4>}
\item \cgreenwhen{Study rare processes at high-$L$ colliders}{<4>}
\item \alert<3>{\cgreenwhen{Hunt for dark matter}{<4>}}
\item Look for kooky neutrino physics
\end{itemize}
}

\end{frame}


\begin{frame}
\frametitle{Combining searches {\rm I}}

  \begin{exampleblock}{Question}
  How do we know which models are in and which are out?
  \end{exampleblock} 

  \visible<2->{
  \begin{block}{Answer}
  Combine the results from different searches
  \end{block}

\begin{columns}[c]
\column{0.5\textwidth}
\footnotesize
\begin{itemize}
\item Simplest method: take different exclusions, overplot them, conclude things are ``allowed'' or ``excluded''
\item Simplest BSM example: the scalar singlet model
\vspace{1cm}
\end{itemize}
\column{0.45\textwidth}
\includegraphics[width=\textwidth]{Fig6a}
\end{columns}
}

  \begin{textblock}{80}(20,75)
  \visible<2->{\tiny(Cline, Kainulainen, PS \& Weniger, \textit{PRD}, 1306.4710)}
  \end{textblock}

\end{frame}


\begin{frame}
\frametitle{Combining searches {\rm II}}

That's all well and good if there are only 2 parameters and few searches\ldots\vspace{3mm}

  \begin{exampleblock}{Question}
  What if there are many different \alert{constraints}?
  \end{exampleblock} 

  \visible<2->
  {
    \begin{columns}[c]
    \column{0.4\textwidth}
      \begin{block}{Answer}
        Combine constraints in a statistically valid way \\$\rightarrow$ composite likelihood
      \end{block}\vspace{10mm}
    \column{0.45\textwidth}
      \includegraphics[width=\textwidth]{Fig3b}
    \end{columns}  
  }

  \begin{textblock}{80}(20,75)
  \visible<2->{\tiny(Cline, Kainulainen, PS \& Weniger, \textit{PRD}, 1306.4710)}
  \end{textblock}

\end{frame}


\begin{frame}
\frametitle{Combining searches {\rm III}}

That's all well and good if there are only 2 parameters and few searches\ldots\vspace{3mm}

  \begin{exampleblock}{Question}
  What if there are many \alert{parameters}?
  \end{exampleblock}

\visible<2->{
\begin{block}{Answer}
Need to 
\begin{itemize}
\item scan the parameter space (smart numerics)
\item interpret the combined results (Bayesian / frequentist)
\item project down to parameter planes of interest (marginalise / profile)
\end{itemize}
$\rightarrow$ \alert{global fits}
\end{block}}\vspace{3mm}

\end{frame}


\begin{frame}
\frametitle{Know your (SUSY) scans}

\begin{columns}[c]
\column{0.3\linewidth}
\textbf{Global fits:}\\
\footnotesize\vspace{2mm}
Quantitative?\\
\hspace{2mm}per-point: always\\
\hspace{2mm}overall: always\vspace{15mm}

\textbf{Not global fits:}\\
\footnotesize\vspace{2mm}
Quantitative?\\
\hspace{2mm}per-point: sometimes\\
\hspace{2mm}overall: never\vspace{5mm}


\column{0.35\linewidth}
\includegraphics[width=0.8\textwidth, trim = 40 40 0 40, clip=true]{Strege13}\\\vspace{-1mm}
{\tiny Strege et al \textit{JCAP}, 1212.2636}\\\vspace{3mm}
\includegraphics[width=\textwidth]{LSP_Gluino_13_Efficiency}\\
{\tiny Cahill-Rowley et al, 1307.8444}\\\vspace{3mm}

\column{0.35\linewidth}
\includegraphics[width=\textwidth]{MasterCode2012_CMSSM}\\\vspace{-1mm}
{\tiny MasterCode, \textit{EPJC}, 1207.7315}\vspace{3mm}
\includegraphics[width=\textwidth]{PvP1_Outm1_v_Outm2}\\
{\tiny Silverwood, PS, et al, \textit{JCAP}, 1210.0844}\vspace{3mm}

\end{columns}

\end{frame}


\begin{frame}
\frametitle{BSM Model Scanning -- Statistics 101}

  \alert{Goals:} 
  \begin{enumerate} 
    \item Given a particular theory, determine which parameter combinations fit all experiments, and how well \visible<2->{\\\hspace{5.15cm}\cblue{$\implies$ parameter estimation}}
    \item Given multiple theories, determine which fit the data better, and quantify how much better \visible<2->{\cblue{$\implies$ model comparison}}
  \end{enumerate} 
  \vspace{4mm}

\visible<3>{
Why simple IN/OUT analyses are not enough\ldots
\vspace{2mm}

\begin{itemize}
\item Only partial goodness of fit, no measure of convergence, no idea how to generalise to regions or whole space.
\item Frequency/density of models in IN/OUT scans is \\\alert{not} proportional to probability $\implies$ means essentially \alert{nothing}.
\end{itemize}
\vspace{3mm}

}

\end{frame}

\begin{frame}
\frametitle{Putting it all together: global fits}  

  \alert{Issue 1:} Combining fits to different experiments\\
  Relatively easy -- composite likelihood ($\mathcal{L}_1\times\mathcal{L}_2 \equiv \chi^2_1 + \chi^2_2$ for simplest $\mathcal{L}$)
  {\footnotesize\begin{itemize}
    \item{dark matter relic density from WMAP}
    \item{precision electroweak tests at LEP}
    \item{LEP limits on sparticle masses}
    \item{$B$-factory data (rare decays, $b\rightarrow s\gamma$)}
    \item{muon anomalous magnetic moment}
    \item{LHC searches, direct detection (only roughly implemented for now)}
  \end{itemize}}
  \vspace{2mm}

\end{frame}

\begin{frame}
\frametitle{Putting it all together: global fits}  

  \alert{Issue 2:} Including the effects of uncertainties in input data\\
  Easy -- treat them as \emph{nuisance parameters}\vspace{4mm}

  \alert{Issue 3:} Finding the points with the best likelihoods\\
  \cbluewhen{Tough -- MCMCs, nested sampling, genetic algorithms, etc}{<2>}\vspace{4mm}

  \alert{Issue 4:} Comparing theories\\
  \cbluewhen{Depends -- Bayesian model comparison, $p$ values\\
  \hspace{40mm}($TS$ distribution? $\longrightarrow$ coverage???)}{<2>} 

\end{frame}


\section{Progress}

\begin{frame}

  \frametitle{Two different approaches to including astro data in BSM scans}



  \begin{enumerate}

  \item{Just use the published limits on $\langle \sigma v\rangle$ (or $\sigma_\mathrm{SI,SD}$)}

    \begin{itemize}

    \item{Fast -- can cover large parameter spaces}

    \item{Not so accurate -- experimental limits are invariably based on theoretical assumptions, e.g. $b\bar b$ spectrum}

    \item{Full likelihood function almost never available}

    \end{itemize}

  \item\alert<2-3>{Use the data points directly in BSM scans} 

    \begin{itemize}

    \item{Slow -- requires full treatment of instrument profile for each point}

    \item{Accurate -- can test each point self-consistently}

    \item{Allows marginalisation over theoretical assumptions}

    \item{Allows construction of full multi-dimensional likelihood function}

    \end{itemize}

  \visible<3>{\item\alert{(indirect only: use just flux upper limits)}}

  \end{enumerate}

\end{frame}


\subsection{Gamma-rays}

\begin{frame}
\frametitle{Gamma-rays}

Gamma-ray annihilation searches in CMSSM global fits: 
\vspace{3mm}

\begin{columns}[t]

\column{0.54\linewidth}
\only<1>{\cblue{\textit{Fermi}-LAT}}%
\only<2-3>{\cblue{HESS}}

{\footnotesize
\only<1>{Satellite pair conversion telescope}%
\only<2-3>{Air \v{C}erenkov telescope}

\only<1>{Dwarf galaxy Segue 1}%
\only<2-3>{Milky Way$+$Carina$+$Sculptor$+$Sag dwarf}
}%
\vspace{2mm}

\only<1>{\tiny (PS, Conrad et al {\it JCAP}, 0909.3300)}%
\only<2-3>{\tiny(Ripken, Conrad \& PS {\it JCAP}, 1012.3939)}\vspace{2mm}

\only<1>{\includegraphics[height=0.6\textwidth, trim = 20 182 20 220, clip=true]{B50_H0_All_P_lin_2D_profl_7}}%
\only<3>{\includegraphics[height=0.6\textwidth]{noHESS}}%
\only<2>{\includegraphics[height=0.6\textwidth]{allHESS}}

\column{0.6\linewidth}
\only<1>{\footnotesize\vspace{-3mm}
\begin{itemize}
\item Full binned Poissonian likelihood (no $\chi^2$ approximation)
\item Full treatment of PSF \textit{and} energy dispersion (with fast convolution library FLATlib) 
\item Marginalisation over systematic error on effective area
\item Diffuse BG from Fermi-LAT Galprop fits
\item Isotropic BG best-fit isotropic power law
\item $J$-factor from Martinez et al (\textit{JCAP}, 0902.4715; best at the time)
\end{itemize}
}%
\only<2-3>{\footnotesize
\begin{itemize}
\item $\chi^2$-based analysis using public flux limits
\item `Milky Way' = halo just beyond GC (45--150\,pc)
\item Virtual internal bremsstrahlung from co-annihilation strip models caught at high-$E$ by HESS 
\item but: $J$-factors for Sag dwarf rather uncertain
\end{itemize}
}

\end{columns}

\end{frame}


\subsection{Neutrinos}

\begin{frame}
\frametitle{How to find DM with neutrino telescopes}

\begin{columns}[c]
\column{0.75\textwidth}
The short version: \begin{enumerate}
\item \visible<2->{Halo WIMPs crash into the Sun}
\item \visible<3->{Some lose enough energy in the scatter to be gravitationally bound}
\item \visible<4->{Scatter some more, sink to the core}
\item \visible<5->{Annihilate with each other, producing neutrinos}
\item \visible<6->{Propagate+oscillate their way to the Earth, convert into muons in ice/water}
\item \visible<7->{Look for \v{C}erenkov radiation from the muons in \textbf{IceCube}, ANTARES, etc}
\end{enumerate}
\column{0.35\textwidth}
  \visible<2->{\includegraphics[width=0.8\columnwidth]{comic3_1}}\\
  \visible<4->{\includegraphics[width=0.6\columnwidth]{comic3_2}}\\
  \visible<5->{\includegraphics[width=0.6\columnwidth, trim = 0 70 150 0, clip=true]{comic_4}}
\end{columns}

\end{frame}

\begin{frame}
\frametitle{Advanced IceCube Likelihood for Model Testing}

Simplest way to do anything is to first make it a counting problem\ldots

\vspace{3mm}

Compare observed number of events $n$ and predicted number $\theta$ for each model, taking into account error $\sigma_\epsilon$ on acceptance:

\begin{equation}
\label{fancy}
\scriptsize
\Like_\mathrm{num}(n|\theta_\mathrm{BG}+\theta_\mathrm{sig}) = \frac{1}{\sqrt{2\pi}\sigma_\epsilon}\int_0^\infty \frac{(\theta_\mathrm{BG}+\epsilon\theta_\mathrm{sig})^{n}e^{-(\theta_\mathrm{BG}+\epsilon\theta_\mathrm{sig})}}{n!}\frac1\epsilon\exp\left[-\frac{1}{2}\left(\frac{\ln\epsilon}{\sigma_\epsilon}\right)^2\right]\mathrm{d}\epsilon \, .
\end{equation}

\only<1>{Nuisance parameter $\epsilon$ takes into account systematic errors on effective area, etc.  $\sigma_\epsilon\sim20\%$ for IceCube.\vspace{2.17cm}}%
\only<2>{\alert{Then:} upgrade to full unbinned likelihood with number ($\Like_\mathrm{num}$), spectral ($\Like_\mathrm{spec}$) and angular ($\Like_\mathrm{ang}$) bits: 
\begin{equation}
\footnotesize
\Like = \Like_\mathrm{num}(n|\theta_{\mathrm{signal+BG}})\prod_{i=1}^{n} \Like_{\mathrm{spec},i}\,\Like_{\mathrm{ang},i}
\end{equation}

{\alert{All available in DarkSUSY v5.0.6 and later: }{\color[rgb]{0.1, 0.0, 0.6}\href{www.darksusy.org}{www.darksusy.org}}
\vspace{3mm}}}

\end{frame}


\begin{frame}
\frametitle{CMSSM model reconstruction with IceCube event data}

Benchmark recovery with 22-string IceCube WIMP-search neutrino events + full likelihood:
\vspace{3mm}

{\centering\cblue{Mock signal}: 60 events, $m_\chi=500$\,GeV,  100\% $\chi\chi\rightarrow W^+W^-$}
\vspace{3mm}

\begin{columns}[t]
\column{0.58\linewidth}
  \includegraphics[width=0.85\textwidth]{recon2D_4}
\column{0.58\linewidth}
  \includegraphics[width=0.85\textwidth]{recon2D_5}
\end{columns}

\begin{textblock}{80}(30,80)
  {\tiny(PS, Savage, Edsj\"o \& The IceCube Collaboration, \textit{JCAP}, 1207.0810)}
\end{textblock}

\end{frame}


\begin{frame}
\frametitle{Example of Combined Direct + Indirect + LHC constraints}

\Large

\visible<1-4>{Base Observables} \visible<2-4>{$+$ XENON-100}\only<2>{ (2011)} \only<3-4>{$+$ CMS\,5\,fb$^{-1}$}

{\centering \visible<4>{$+$ projected IC86-DeepCore}}

\vspace{2mm}
\footnotesize
\visible<2-4>{Grey contours correspond to Base Observables \textit{only}}
\vspace{2mm}

\begin{columns}
\column{0.36\textwidth}
\only<1>{\includegraphics[height=0.92\linewidth, trim = 8 0 30 0, clip = true]{marg2D_1_1}}%
\only<2>{\includegraphics[height=0.92\linewidth, trim = 8 0 30 0, clip = true]{marg2D_2_1}}%
\only<3>{\includegraphics[height=0.92\linewidth, trim = 8 0 30 0, clip = true]{marg2D_3_1}}%
\only<4>{\includegraphics[height=0.92\linewidth, trim = 8 0 30 0, clip = true]{marg2D_4_1}}
\column{0.36\textwidth}
\only<1>{\includegraphics[height=0.92\linewidth, trim = 8 0 30 0, clip = true]{marg2D_1_2}}%
\only<2>{\includegraphics[height=0.92\linewidth, trim = 8 0 30 0, clip = true]{marg2D_2_2}}%
\only<3>{\includegraphics[height=0.92\linewidth, trim = 8 0 30 0, clip = true]{marg2D_3_2}}%
\only<4>{\includegraphics[height=0.92\linewidth, trim = 8 0 30 0, clip = true]{marg2D_4_2}}
\column{0.36\textwidth}
\only<1>{\includegraphics[height=0.92\linewidth, trim = 8 0 10 0, clip = true]{marg2D_1_3}}%
\only<2>{\includegraphics[height=0.92\linewidth, trim = 8 0 10 0, clip = true]{marg2D_2_3}}%
\only<3>{\includegraphics[height=0.92\linewidth, trim = 8 0 10 0, clip = true]{marg2D_3_3}}%
\only<4>{\includegraphics[height=0.92\linewidth, trim = 8 0 10 0, clip = true]{marg2D_4_3}}
\end{columns}
\vspace{2mm}

\visible<4>{\textbf{CMSSM, IceCube-22 with 100$\times$ boosted effective area}\\
(kinda like IceCube-DeepCore)}

\end{frame}


\begin{frame}
\frametitle{Prospects for detection in the MSSM-25}
86-string IceCube vs Direct Detection {\scriptsize(points pass $\Omega_\chi h^2$, $b\rightarrow s\gamma$, LEP)}
\vspace{3mm}

\includegraphics[width=1.1\textwidth, trim = 50 0 0 0, clip=true]{MSSM25_DD}

\vspace{3mm}
Many models that IceCube-86 can see are not accessible to direct detection\ldots

\begin{textblock}{80}(75,70)
  {\tiny(Silverwood, PS, et al, \textit{JCAP}, 1210.0844)}
\end{textblock}

\end{frame}

\begin{frame}
\frametitle{Prospects for detection in the MSSM-25}
86-string IceCube vs LHC (very naively)\\
{\scriptsize SMS limits: 7\,TeV, 4.7\,fb$^{-1}$, jets + $E_{T,miss}$; 0 leptons (ATLAS), razor + $M_{\sf T2}$ (CMS)}

\includegraphics[width=1.1\textwidth, trim = 50 0 0 0, clip=true]{MSSM25_LHC}

Many models that IceCube-86 can see are also not accessible at colliders.

\begin{textblock}{80}(75,70)
  {\tiny(Silverwood, PS, et al, \textit{JCAP}, 1210.0844)}
\end{textblock}

\end{frame}

\subsection{CMB constraints}

\begin{frame}
\frametitle{Generalised DM CMB likelihood functions}

Simple CMB likelihood function, for
\begin{itemize}
\item Any combination of annihilation or decay channels
\item Any dark matter mass 
\item Any decay lifetime/annihilation cross-section
\end{itemize}
$\rightarrow$ just requires interpolating one number in a table.\vspace{5mm}

\visible<2-3>{
$f_\mathrm{eff}$ for annihilation:
\begin{equation}\footnotesize
 \ln\,\mathcal{L}(\langle\sigma v\rangle|m_\chi,r_i) = -\frac12 f_{\rm eff}^2(m_\chi,r_i) \lambda_1 c_1^2\,
 \left(\frac{\langle\sigma v\rangle}{2\times 10^{-27}{\rm cm}^3{\rm s}^{-1}}\right)^2\,\left(\frac{\rm GeV}{m_\chi}\right)^2
\label{loglike2}
\end{equation}
}

\visible<3>{
$\eta$ for decay:
\begin{equation}\footnotesize
\ln\, \mathcal{L}(\tau|m_\chi,r_i) = -\frac12\left(\delta\Omega\over\Omega_\mathrm{DM}\tau\right)^2\eta^2(\tau,m_\chi,r_i)
\end{equation}
}


\begin{textblock}{70}(15,48)
  \visible<4>{
    \begin{columns}
    \column{0.6\linewidth}
    \includegraphics[width=1.1\textwidth]{planck-limit}
    \column{0.1\linewidth}
    \column{0.6\linewidth}
    \includegraphics[width=1.1\textwidth]{electrons-w-Planck}
    \end{columns}
  }
\end{textblock}

\begin{textblock}{80}(15,49)
  \visible<1>
  {
    Cline \& PS, \textit{JCAP}, 1301.5908, using
    \begin{itemize}
      \item CMB energy deposition from\begin{itemize}
        \footnotesize \item[-] Slatyer (\textit{PRD}, 1211.0283)
        \item[-] Finkbeiner et al (\textit{PRD}, 1109.6322)
      \end{itemize}
      \item PYTHIA annihilation/decay spectra from
      \begin{itemize}
        \footnotesize \item[-] Cirelli et al (PPPC4DMID; \textit{JCAP}, 1012.4515)
      \end{itemize}
    \end{itemize}
  }
\end{textblock}

\end{frame}

\section{Future Challenges}

\subsection{Respectable LHC likelihoods}

\begin{frame}
\frametitle{The LHC likelihood monster}

\visible<1->{
\begin{exampleblock}{Time per point:}
$\mathcal{O}(minute)$ in \alert{best} cases
\end{exampleblock}
}

\visible<2->{
\begin{exampleblock}{Time per point for global fits to converge:}
$\mathcal{O}(seconds)$ in \alert{worst} cases
\end{exampleblock}
}

\visible<3->{
\begin{exampleblock}{Challenge:}
About 2 orders of magnitude too slow to actually include LHC data in global fits properly
\end{exampleblock}
}

\end{frame}

\begin{frame}
\frametitle{Taming the LHC monster}

\only<1,2>{
\begin{exampleblock}{Zeroth Order Response:}
``Stuff it, just use the published limits and ignore the dependence on other parameters''
\end{exampleblock}

\visible<2>{
\vspace{5mm}
Obviously naughty -- plotted limits assume CMSSM, and fix two of the parameters
\begin{itemize}    
  \item Don't really know dependence on other parameters
  \item Don't have a likelihood function, just a line
  \item Can't use this at all for non-CMSSM global fits -- e.g. MSSM-25
\end{itemize}

\vspace{5mm}
SuperBayeS
}}  

\only<3,4>{
\begin{exampleblock}{First Order Response:}
``Test if things depend on the other parameters (hope not), re-simulate published exclusion curve''
\end{exampleblock}

\visible<4>{
\vspace{5mm}
Not that great, but OK in some cases
\begin{itemize}    
  \item At least have some sort of likelihood this time
  \item Still a bit screwed if things do depend a lot on other parameters, but
  \item allows (potentially shaky) extrapolation, also to non-CMSSM models
\end{itemize}

\vspace{5mm}
Fittino, Mastercode
}}

\only<5,6>{
\begin{exampleblock}{Second Order Response:}
``That's ridiculous.  I've never met a calculation I can't speed up.  There must be some way to have my cake and eat it too''
\end{exampleblock}

\visible<6>{
\vspace{5mm}
Maybe -- this is the challenge.
\begin{itemize}    
  \item Interpolated likelihoods (how to choose nodes?)
  \item Neural network functional approximation (how to train accurately?)
  \item Some sort of smart reduction based on event topology? 
  \item Something else?
\end{itemize}

\vspace{5mm}
Bal\'azs, Buckley, Farmer, White et al (1106.4613, 1205.1568)  
}}

\end{frame}


\subsection{Statistical/numerical issues }

\begin{frame}
\frametitle{Scanning algorithms}

\cblue{Convergence remains an issue, especially for profile likelihood}\\
Messy likelihood $\implies$ best-fit point can be (and often is) easily missed {\tiny(Akrami, PS et al {\it JHEP}, 0910.3950, Feroz et al {\it JHEP}, 1101.3296)}
      \begin{itemize}\footnotesize
                 \item frequentist CLs are off, as isolikelihood levels are chosen incorrectly
                 \item can impact coverage (overcoverage, or masking of undercoverage due to non-$\chi^2$ $TS$ distribution)
                 \item need to use multiple priors and scanning algorithms (one optimised for profile likelihoods?)  
      \end{itemize}\vspace{-2mm}
\includegraphics[width=0.5\textwidth]{MN_m0mhf_MNlevels}	
\includegraphics[width=0.5\textwidth]{GA_m0mhf_MNlevels}	


\end{frame}


\begin{frame}
\frametitle{Coverage}

{\scriptsize
[Statistical aside]\\\vspace{1mm}
\textbf{Test statistic}: a measure on data used to construct statistical tests (e.g. $\chi^2$, ln$\mathcal{L}$, etc.)\\
\textbf{Coverage}: the percentage of the time that a supposed `$x\%$' confidence region\\ actually contains the true value\\
\begin{itemize}
\item Distribution of the test statistic and design of the test it's used in determine coverage.
\item $p$-value calculation \textit{requires} the test statistic distribution to be well known.
\end{itemize}
}\vspace{3mm}

\cblue{We don't \textit{*really*} know the distribution of our test statistic in BSM global fits, as it is too expensive to Monte Carlo}
      \begin{itemize}\footnotesize
                 \item coverage is rarely spot-on unless mapping from parameters to data-space is linear \\
                 {\tiny(Akrami, Savage, PS et al {\it JCAP}, 1011.4297, Bridges et al {\it JHEP}, 1011.4306, Strege et al {\it PRD}, 1201.3631)}
                 \item $p$-value assessments of goodness of fit should be viewed with serious scepticism ($\rightarrow$MasterCode)
                 \end{itemize}

\end{frame}


\subsection{Parameter space $\rightarrow$ Theory space}

\begin{frame}
\frametitle{CMSSM, SMS $\ne$ BSM}

(SMS = Simplified Model Spectrum)
\vspace{3mm}

Want to do model comparison to actually work out which theory is right\ldots
\vspace{3mm}

\begin{exampleblock}{Challenge:}
How do I easily adapt a global fit to different BSM theories?
\end{exampleblock}

\visible<2>{
Somehow, we must recast things quickly to a new theory 
\begin{itemize}
\item data
\item likelihood functions
\item scanning code `housekeeping'
\item even predictions
\end{itemize}
$\implies$ a new, very abstract global fitting framework
}

\end{frame}

\begin{frame}
\frametitle{Hitting the wall}

Issues with current global fit codes:
\begin{itemize}
\item Strongly wedded to a few theories (e.g. constrained MSSM / mSUGRA)
\item Strongly wedded to a few theory calculators
\item All datasets and observables basically hardcoded
\item Rough or non-existent treatment of most experiments (astroparticle + collider especially)
\item Sub-optimal statistical methods / search algorithms
\item $\implies$ \textit{already hitting the wall on theories, data \& computational methods}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{\textbf{GAMBIT}: a \textit{second-generation} global fit code}

GAMBIT: \alert{G}lobal \alert{A}nd \alert{M}odular \alert{B}SM \alert{I}nference \alert{T}ool
\vspace{5mm}

Overriding principles of GAMBIT: flexibility and modularity
\begin{itemize}
\item General enough to allow fast definition of new datasets and theoretical models
\item Plug and play scanning, physics and likelihood packages
\item Extensive model database -- not just small modifications to constrained MSSM (NUHM, etc), and not just SUSY!
\item Extensive observable/data libraries (likelihood modules)
\item Many statistical options -- Bayesian/frequentist, likelihood definitions, scanning algorithms
\item A smart and \textit{fast} LHC likelihood calculator
\item Massively parallel
\item Full open-source code release
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{The GAMBIT Collaboration}

22 Members, 13 Institutes \\
8 Experiments, 3 major theory codes \vspace{3mm}

\begin{tabular}{l l}
\textbf{Fermi-LAT} & \small J.\ Conrad, J.\ Edsj\"o, G.\ Martinez, P.\ Scott{\footnotesize$^\dagger$}\\
\textbf{IceCube} & \small J.\ Edsj\"o, C.\ Savage, P.\ Scott\\
\textbf{ATLAS} & \small A.\ Buckley, P.\ Jackson, C.\ Rogan, A.\ Saavedra, M.\ White\\
\textbf{LHCb} & \small N.\ Serra\\
\textbf{HESS} & \small J.\ Conrad \\
\textbf{AMS-02} & \small A.\ Putze\\
\textbf{CTA} & \small C. Bal\'azs, T.\ Bringmann, J.\ Conrad, M.\ White\\
\textbf{DARWIN} & \small J.\ Conrad\\
\textbf{Theory} & \small C. Bal\'azs, T.\ Bringmann, J.\ Cornell, L.-A.\ Dal, J.\ Edsj\"o, \\
                & \small B.\ Farmer, A.\ Krislock, A.\ Kvellestad, F.N.\ Mahmoudi, \\
                & \small A.\ Raklev, C.\ Savage, P.\ Scott, C.\ Weniger, M.\ White \\	
\end{tabular}
{\tiny$^\dagger$PI}

\end{frame}


\begin{frame}
\frametitle{Closing remarks}

\begin{itemize}
\item{Robust analysis of dark matter and BSM physics requires multi-messenger global fits}
\item{Lots of interesting astroparticle observables to include in global fits}
\item{Quite a bit of technical (statistical/computational) detail to worry about}
\item{GAMBIT is coming}
\end{itemize}


\end{frame}

\appendix

\section{Backup Slides}

\begin{frame}
\frametitle{Prospects for detection in the MSSM-25}
86-string IceCube vs Gamma Rays

\begin{center}
\includegraphics[width=0.65\textwidth]{MSSM25_ID}
\end{center}

Many models that IceCube-86 can see are not accessible by other indirect probes\ldots

\begin{textblock}{80}(75,72)
  {\tiny(Silverwood, PS, et al, \textit{JCAP}, 1210.0844)}
\end{textblock}

\end{frame}

\begin{frame}
\frametitle{Prospects for detection in the MSSM-25}
Gaugino fractions
\vspace{3mm}

\includegraphics[width=1.1\textwidth, trim = 50 0 0 0, clip=true]{MSSM25_Gaugino}

\vspace{3mm}
Mainly mixed models, a few Higgsinos

\begin{textblock}{80}(75,72)
  {\tiny(Silverwood, PS, et al, \textit{JCAP}, 1210.0844)}
\end{textblock}

\end{frame}

\end{document}

