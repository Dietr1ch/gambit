#!/usr/bin/env python
#
#  GUM: GAMBIT Universal Models
#  ****************************
#  \file
#
#  Main GUM script
#
#  *************************************
#
#  \author Sanjay Bloor
#          (sanjay.bloor12@imperial.ac.uk)
#  \date 2017, 2018, 2019... forever
#
#  \author Tomas Gonzalo
#          (tomas.gonzalo@monash.edu)
#  \date 2019 July
#
#  **************************************


import sys
import os
if not sys.version_info[0] == 2 or sys.version_info[1] < 6:
    sys.exit('Please use python 2.6 or later -- but not python 3.')

import argparse
import numpy as np

from src import *

from collections import defaultdict

parser = argparse.ArgumentParser(description="From Lagrangian to scans: GUM "
                                             "(GAMBIT Universal Models)")

# Optional command-line arguments
parser.add_argument('-f', '--file', type=str,
                      help="Specify input .GUM file.")
parser.add_argument("-d", "--dryrun", action='store_true',
                    help="GUM will perform a dry run, not saving any output.")
parser.add_argument("-r", "--reset", type=str,
					help="GUM will reset GAMBIT back to a previous version, given an input .mug file generated by GUM.")
# Retrieve command-line arguments
args = parser.parse_args()

# Don't let the user create and reset at the same time
if args.file and args.reset:
	raise GumError(("\n\n\tYou may use only one of the "
					"--file or --reset flags at any one time!"))

if args.reset and args.dryrun:
	raise GumError(("\n\n\tYou have requested a dryrun and a reset together:"
					" this is not supported."))

# Input .gum file.
if args.file:

    if args.dryrun:
        print("**********************************************************")
        print("GUM called with a dry run -- will not be writing to files!")
        print("**********************************************************")

    # Parse the .gum file
    inputs = check_gum_file(args.file)
    gum, output_opts = fill_gum_object(inputs)

    # First, check the required backends are installed
    check_backends(output_opts)

    # Create the output directory for all generated files for this model
    output_dir = os.path.join(os.getcwd(),"Outputs", gum.name)
    mkdir_if_absent(output_dir)

    # Check for a DM candidate
    if gum.dm_pdg:
        darkbit = True
    else:
        darkbit = False
        print(("No DM candidate requested -- no DarkBit routines will "
               "be written.\n"))

    # Check whether we need to generate DecayBit output or not
    if output_opts.ch or output_opts.spheno:
        decaybit = True
    else:
        decaybit = False

    # Check whether we need to generate new ColliderBit code or not
    if output_opts.pythia:
        colliderbit = True
    else:
        colliderbit = False

    # Check to see if any of the proposed new files will
    # clash with any existing files.
    check_for_existing_entries(gum.name, darkbit, colliderbit, output_opts)

    """
    FEYNRULES/SARAH
    """
    if gum.math == 'feynrules':

        from lib.libfr import *

        # Create the output directory for all generated FeynRules files
        mkdir_if_absent(output_dir + "/FeynRules")

        options = FROptions("feynrules", gum.mathname, gum.base_model, gum.restriction, gum.LTot)
        partlist = FRVectorOfParticles()
        paramlist = FRVectorOfParameters()
        outputs = FROutputs()
        backends = FRBackends()
        backends.extend(x for x in output_opts.bes())

        # Hit it
        all_feynrules(options, partlist, paramlist, outputs, backends)

        # Get a list of all non-SM particles
        fr_bsm = [x for x in partlist if x.SM() is False]

        # Make all BSM particles work with GUM's native Particle class.
        # Find out if the spectrum needs a dependency on StandardModel_Higgs.
        bsm_particle_list, add_higgs = fr_part_to_gum_part(fr_bsm)

        # Initialise all of the model parameters, for writing spectra,
        # model files - remove all SM stuff from parameter list.
        parameters = fr_params(paramlist, add_higgs)

    elif gum.math == 'sarah':

        from lib.libsarah import *

        # Create the output directory for all generated SARAH files
        mkdir_if_absent(output_dir + "/SARAH")

        options = SARAHOptions("sarah", gum.mathname, '', '', '')
        partlist = SARAHVectorOfParticles()
        paramlist = SARAHVectorOfParameters()
        flags = SARAHMapOfFlags()
        outputs = SARAHOutputs()
        backends = SARAHBackends()
        mixings = SARAHMixings()
        sphenodeps = SARAHVectorOfParameters()
        error = SARAHError()
        backends.extend(x for x in output_opts.bes())

        # Hit it
        all_sarah(options, partlist, paramlist, outputs, backends, flags, mixings, sphenodeps, error)

        if error.is_error():
            raise GumError( error.what() )

        # Get a list of all non-SM particles
        sarah_bsm = [x for x in partlist if x.SM() is False]

        # Make all BSM particles work with GUM's native Particle class.
        bsm_particle_list, add_higgs = sarah_part_to_gum_part(sarah_bsm)

        # Initialise all of the model parameters, for writing spectra,
        # model files - remove all SM stuff from parameter list.
        parameters = sarah_params(paramlist, add_higgs)

    print("Finished extracting parameters from " + gum.math + ".")

    # Get the parameters sorted by block
    blockparams = sort_params_by_block(parameters)
    # Dict of which parameters are real.
    reality_dict = parameter_reality_dict(parameters)

    # Clean up calchep files from SARAH/FeynRules, then copy them to the GAMBIT backendDir
    # TODO make this go to the Outputs dir instead, then copy (or ln -s?) from there to CalcHEP later.
    if output_opts.ch or output_opts.mo:
        clean_calchep_model_files(outputs.get_ch(), gum.name)

    # Move MadGraph files from SARAH/FeynRules to contrib/MadGraph/models
    # TODO make this go to the Outputs dir instead, then ln -s from there to MadGraph later.
    if output_opts.pythia:
        copy_madgraph_files(outputs.get_mg(), gum.name)
    
    """
    # If we have both UFO and MDL files, run the files through ufo2mdl to compare outputs.
    # This will, e.g., add 4-fermion interactions to CalcHEP where FeynRules fails to do so.
    if output_opts.ch and output_opts.pythia:
    # TODO make this depend on opts.ufo also.
    compare(outputs.get_ch(), outputs.get_mg()):

    todo: figure out what to return 
    """

    # Create defaultdict for reset file.
    # There are 2 nodes: new_files and amended_files.
    # todo: add backend patches (e.g. pythia stuff) to reset_contents
    # todo: write routines to: ADD_CAPABILITY, ADD_FUNCTION, DELETE_CAPABILITY, DELETE_FUNCTION, so ordering is not important.
    reset_contents = defaultdict(lambda: defaultdict(list))

    """
    MADGRAPH
    """

    # TODO this should eventually be conditional on a dedicated MadGraph flag, 
    # as it will eventually be needed for MadDM and other things than just Pythia.
    if output_opts.pythia:  

        if output_opts.collider_processes:
            # Link MadGraph files output by SARAH/FeynRules to MadGraph's contrib/MadGraph/models
            # still TODO

            # Create the output directory for all generated MadGraph files
            mg5_output_dir = output_dir + "/MadGraph5_aMC"
            mkdir_if_absent(mg5_output_dir)

            # Clear and remake the copy of Pythia that will receive the matrix element code produced by MadGraph
            pristine_pythia_dir = os.path.join(os.getcwd(),'contrib','pythia')
            new_pythia_dir = mg5_output_dir + "/Pythia_patched"
            remove_tree_quietly(new_pythia_dir)
            copy_tree(pristine_pythia_dir, new_pythia_dir)

            # Create the MadGraph script
            # TODO determine all possible BSM processes automatically when collider_processes is missing from the .gum file
            make_madgraph_script(mg5_output_dir, gum.name, output_opts.collider_processes, 
                                 output_opts.multiparticles)

            # Add MadGraph to path and import the python interface
            mg5_dir = os.path.join(os.getcwd(),'contrib','MadGraph')
            sys.path.append(mg5_dir)
            import madgraph.interface.master_interface as mi

            # Run MadGraph
            call_madgraph(mg5_dir, mg5_output_dir, mi)

        else:
            raise GumError(("Pythia output requested but no collider_processes specified.\n"
                            "Please amend your .gum file!"))

    print("")
    print("Finished running external codes. Now writing proposed GAMBIT code.")

    """
    REGISTRATION OF NEW PARTICLES
    """

    # Initialise DM particle from particle list
    if darkbit:
        for i in xrange(len(partlist)):
            part = partlist[i]
            # If specified, initialise the DM candidate
            if part.pdg() == gum.dm_pdg:
                dm = Particle(part.name(), part.antiname(),
                              part.spinX2(), part.pdg(),
                              part.mass())

    # If we haven't found the DM candidate in the particle list...
    if darkbit and not dm:
        raise GumError(("\n\nThe DM candidate specified has not been found "
                        "in the {0} file. Please check your .gum "
                        "file as well as the {0} file.").format(gum.math))

    print("Adding new model {0} to GAMBIT.").format(gum.name)

    # Need to know how GAMBIT refers to each particle. Scrape information
    # from particle_database.yaml.
    gambit_pdgs, decaybit_dict = get_gambit_particle_pdg_dict()

    # Check every new particle is in the database
    missing_parts = check_all_particles_present(partlist, gambit_pdgs)

    if len(missing_parts) != 0:
        if args.dryrun:
            print("GUM won't add to the particle database for a dryrun.")
            print("GUM will fail shortly. That's a promise.")
        else:
            # Add new particles to the database.
            add_new_particleDB_entry(missing_parts, gum.dm_pdg)
            # Mow reload the dictionaries
            gambit_pdgs, decaybit_dict = get_gambit_particle_pdg_dict()

    # Grab the antiparticles
    antiparticle_dict = get_antiparticles(partlist)
    
    """
    MODELS
    """
    # TODO: should the BSM particle masses be model parameters?
    # S.B. 4/10
    # For now I've made this for FeynRules only, since the masses 
    # aren't generated by spectra. For SARAH however we only want to use 
    # the fundamental Lagrangian parameters. 

    # Once we've got the GAMBIT dictionaries, we can add the new parameters
    # as pole masses to the existing list.
    
    # For FeynRules, use masses of particles as input parameters
    if gum.math == 'feynrules':
        # Add all of the masses of BSM particles as pole masses. 
        # Also removes all duplicate entries that can arise
        # for e.g. multiplets with same tree-level masses
        parameters = add_masses_to_params(parameters, bsm_particle_list, 
                                          gambit_pdgs, add_higgs)
    
    # Get the model parameters out of the parameter list
    model_parameters = get_model_parameters(parameters, partlist);

    model_header = add_to_model_hierarchy(gum.spec, gum.name, model_parameters)
    spec_contents = write_spectrumcontents(gum.name, model_parameters)
    subspec_wrapper = write_subspectrum_wrapper(gum.name, model_parameters)
    reg_spec, reg_spec_num = add_to_registered_spectra(gum.name)

    """
    SPHENO
    """

    if output_opts.spheno:

        # SARAH removes hyphens and underscores for .f90 files...
        clean_model_name = gum.name.replace('-','').replace('_','')

        # Move SPheno files from SARAH to Outputs.
        copy_spheno_files(clean_model_name, output_dir, SPHENO_DIR, outputs.get_sph())

        # Pristine and patched SPhenos
        pristine_spheno_dir = output_dir + "/SPheno"
        patched_spheno_dir = output_dir + "/SPheno_patched"

        # Patch the files
        print("Patching SPheno...")
        patch_spheno(clean_model_name, patched_spheno_dir, flags)

        # Create a diff vs. the out of the [SARAH]box
        write_backend_patch(output_dir, pristine_spheno_dir, patched_spheno_dir, "spheno_"+gum.name.lower(), SPHENO_VERSION)

        # Get the defs of any SPheno dependencies
        deps = spheno_dependencies(sphenodeps)

        # Write frontends for SPheno. That's a lotta scrapin'.
        print("Writing SPheno frontends.")
        spheno_src, spheno_header = write_spheno_frontends(clean_model_name, parameters, bsm_particle_list, 
                                                           flags, patched_spheno_dir, output_dir, 
                                                           blockparams, gambit_pdgs, mixings, reality_dict, 
                                                           deps)

        # Write out the frontend header and source files
        write_backend_frontends(output_dir, gum.name, "SARAHSPheno", SPHENO_VERSION, spheno_header, spheno_src)
 
        # TODO add this to reset_dict (^amend that function above)

        # DecayBit entry
        print("Writing SPheno module functions for DecayBit.")
        spheno_decay_src, spheno_decay_header = write_spheno_decay_entry(clean_model_name)


    """
    CALCHEP
    """

    # Obtain all model interactions from CalcHEP model files.
    # Also grab the PDG codes of each particle and the names of their masses.
    # We'll want to use this for writing the micromegas frontend too.
    if output_opts.ch or output_opts.mo:
        
        (interactions, calchep_pdg_codes, calchep_masses,
        calchep_widths, aux_particles) = get_vertices(outputs.get_ch())

    if output_opts.ch:

        # Obtain all 3-point vertices that are BSM.
        three_pi_bsm = [interaction.particles for interaction in interactions
                        if interaction.num_particles() == 3
                        and interaction.is_sm() == False]

        three_body_decays = decay_sorter(three_pi_bsm, aux_particles, antiparticle_dict)

        print("Writing CalcHEP module functions for DecayBit")
        # CalcHEP specific decays
        decaybit_src_ch = ""

        """
        PART I: DECAYBIT
        """
        # TODO: if we have SPheno output, do we /really/ want 
        # to provide CalcHEP DecayBit entries too? 

        # Pass all interactions by first PDG code needed.
        for i in xrange(len(three_body_decays)):
            decaybit_src_ch += write_decaytable_entry_calchep(
                                                      three_body_decays[i],
                                                      gum.name,
                                                      calchep_pdg_codes,
                                                      gambit_pdgs,
                                                      decaybit_dict)

        decay_roll, new_decays = \
            write_decaybit_rollcall_entry_calchep(gum.name, gum.spec,
                                                  three_body_decays,
                                                  decaybit_dict, gambit_pdgs)

        all_decays_src, all_decays_header = \
            amend_all_decays_calchep(gum.name, gum.spec, new_decays)

    """
    COLLIDERBIT
    """
    if colliderbit:

        print("Writing new module functions for ColliderBit")

        # Create the output directory for all generated ColliderBit sources
        cb_output_dir = output_dir + "/ColliderBit"
        mkdir_if_absent(cb_output_dir)

        # Write ColliderBit headers and sources for the new model
        new_colliderbit_model(cb_output_dir, gum.name)

    """
    DARKBIT
    """
    if darkbit:

        pc = None
        sv = None
        dd = None
        mo = None
        mdm = None
        ann_products = None
        propagators = None

        print("Writing new module functions for DarkBit")

        # Write process catalogue output
        pc = True

        # Conditional on CalcHEP files being made.
        if output_opts.ch:

          sv = True

          # Obtain all three-field vertices
          three_f = [interaction.particles for interaction in interactions
                     if interaction.num_particles() == 3]
          # And all 4-point BSM vertices
          four_f = [interaction.particles for interaction in interactions
                    if interaction.num_particles() == 4
                    and interaction.is_sm() == False]

          # Get all annihilation products and propagators involved in
          # DM + DM -> X + Y at tree-level
          ann_products, propagators = sort_annihilations(dm, three_f, four_f)

        # If micrOMEGAs requested
        if output_opts.mo:

            print("Patching micrOMEGAs...")

            # Pristine and patched micrOMEGAs
            pristine_mo_dir = output_dir + "/micrOMEGAs"
            patched_mo_dir = output_dir + "/micrOMEGAs_patched"

            # Copy micromegas files to Backend patches from the cleaned
            # CalcHEP directory
            copy_micromegas_files(gum.name)

            # Add the patch file to the directory too
            patch_micromegas(gum.name, reset_contents)

            print("Writing micrOMEGAs interface for DarkBit.")

            mo_src = write_micromegas_src(gum.name, gum.spec, gum.math, 
                                          parameters, bsm_particle_list, 
                                          gambit_pdgs, calchep_masses, 
                                          calchep_widths)
            mo_head = write_micromegas_header(gum.name, gum.math, parameters)


        """
        # If MadDM requested
        if output_opts.mdm:

            print("Writing MadDM interface for DarkBit.")
            # TODO: backend MadDM...

        # TODO - pass over required information for direct detection
        # DirectDM?
        if output_opts.ddm:

            print("Writing direct detection interface for DarkBit.")

            # Write direct detection interface
            dd = True
        """

        darkbit_src = write_darkbit_src(dm, pc, sv, dd, ann_products,
                                        propagators, gambit_pdgs, gum.name,
                                        calchep_pdg_codes, bsm_particle_list)

        pc_cap, dd_cap, dmid_cap = write_darkbit_rollcall(gum.name, pc, dd)

    """
    SPECBIT
    """

    spectrum_src = ""

    if output_opts.spheno:
        print("Writing SPheno SpecBit interface...")
        spectrum_src = write_spheno_spectrum_src(gum.name)
        
    # TODO: elif output_opts.flexiblesusy(): ...
    else:
        print("Writing basic container SpecBit interface...")
        # TODO: determine when "simple_SMinputs" needed from FR/S model file
        spectrum_src = write_basic_spectrum(gum.name, parameters, gum.spec, add_higgs)

    spectrum_header = write_spectrum_header(gum.name, add_higgs, output_opts.spheno)
    spec_rollcall = write_specbit_rollcall(gum.name)

    """
    VEVACIOUS
    """

    if output_opts.vev:

        # Currently require SPheno output for vevacious since we need MINPAR etc. 
        # TODO factorise this a bit
        if not output_opts.spheno:
            raise GumError(("Currently gum needs SPheno output to be able to produce Vevacious"
                            " output. Please change your .gum file."))
        vev_src = write_vevacious_src(gum.name, outputs.get_vev(), gum.spec, 
                                      blockparams)



    """
    BACKENDS
    """

    print("Writing new frontends and patches for backend codes.")

    # Conditional on CalcHEP files being made.
    if output_opts.ch:
        ch_src_sl, ch_src_pl, ch_head = add_calchep_switch(gum.name, gum.spec)

    # Have we created a new Pythia backend?
    if output_opts.pythia:
        print("  Patching Pythia to inject new matrix elements into its Process Container and shared library.")
        fix_pythia_lib(gum.name, new_pythia_dir, output_opts.pythia_groups)
        print("  Creating a diff vs original version of Pythia.")
        write_backend_patch(output_dir, pristine_pythia_dir, new_pythia_dir, "pythia_"+gum.name.lower(), "8."+base_pythia_version)
        print("  Writing an additional patch for the new version of Pythia.")
        patch_pythia_patch(parameters, gum.name, reset_contents)
        print("  Creating a BOSS config file for Pythia_"+gum.name+".")
        write_boss_config_for_pythia(gum.name, output_dir)
        print("  Creating a cmake entry for Pythia"+gum.name+".")
        add_new_pythia_to_backends_cmake(gum.name, output_dir)
        print("  Setting the default version of Pythia_"+gum.name+" for BOSSed classes to 8."+base_pythia_version)
        write_new_default_bossed_version("Pythia_"+gum.name, "8."+base_pythia_version, output_dir)

    # Stop now if we're just doing a dry run
    if args.dryrun:
        print("")
        print("Dry run finished.")
        print("")
        exit()
    #TODO: for now do not write anything, ever
    #exit()

    print("")
    print("Now putting the new code into GAMBIT.")

    # All file writing routines HERE, once everything has gone okay.


    # Models
    m = "Models"
    write_file("models/" + gum.name + ".hpp", m, model_header, reset_contents)
    write_file("SpectrumContents/" + gum.name + ".cpp", m, spec_contents, reset_contents)
    write_file("SimpleSpectra/" + gum.name + "SimpleSpec" + ".hpp", m, subspec_wrapper, reset_contents)
    amend_file("SpectrumContents/RegisteredSpectra.hpp", m, reg_spec, reg_spec_num, reset_contents)

    # SpecBit
    m = "SpecBit"
    write_file("SpecBit_" + gum.name + ".cpp", m, spectrum_src, reset_contents)
    write_file("SpecBit_" + gum.name + "_rollcall.hpp", m, spectrum_header, reset_contents)
    num = find_string("SpecBit_rollcall.hpp", m, "SpecBit_tests_rollcall.hpp")[1]
    amend_file("SpecBit_rollcall.hpp", m, spec_rollcall, num, reset_contents)

    # DecayBit
    if decaybit:
        m = "DecayBit"
        # Append to the end of DecayBit -- just before all_decays
        num = find_string("DecayBit.cpp", m, "void all_decays")[1]
        if output_opts.ch:
            amend_file("DecayBit.cpp", m, decaybit_src_ch, num-4, reset_contents)
            for i in xrange(len(decay_roll)):
                if find_capability(decay_roll[i][0], m)[0]:
                    amend_rollcall(decay_roll[i][0], m, decay_roll[i][1], reset_contents)
                else:
                    num = find_string("DecayBit_rollcall.hpp", m, "#define CAPABILITY decay_rates")[1]
                    amend_file("DecayBit_rollcall.hpp", m, decay_roll[i][1], num-2, reset_contents)
            if len(new_decays) > 0:
                num = find_string("DecayBit_rollcall.hpp", m, "MODEL_CONDITIONAL_DEPENDENCY(MSSM_spectrum")[1]
                amend_file("DecayBit_rollcall.hpp", m, all_decays_header, num-1, reset_contents)
                num = find_string("DecayBit.cpp", m, "decays(\"omega\")")[1]
                amend_file("DecayBit.cpp", m, all_decays_src, num, reset_contents)

    # DarkBit
    if darkbit:
        m = "DarkBit"
        write_file(gum.name + ".cpp", m, darkbit_src, reset_contents)
        if pc:
            amend_rollcall("TH_ProcessCatalog", m, pc_cap, reset_contents)
        if dd:
            amend_rollcall("DD_couplings", m, dd_cap, reset_contents)
        amend_rollcall("DarkMatter_ID", m, dmid_cap, reset_contents)

    # ColliderBit
    m = "ColliderBit"
    if output_opts.pythia:
        copy_file("models/"+gum.name+".hpp", m, output_dir, reset_contents, existing = False)
        copy_file("models/"+gum.name+".cpp", m, output_dir, reset_contents, existing = False)

    # Backends
    m = "Backends"
    # cmake
    if output_opts.pythia:
        copy_file("backends.cmake", "cmake", output_dir, reset_contents, existing = True)
    # CalcHEP
    if output_opts.ch:
        f = "frontends/CalcHEP_3_6_27.cpp"
        num = find_string(f, m, "setModel(modeltoset, 1)")[1]
        amend_file(f, m, ch_src_sl, num-2, reset_contents)
        num = find_string(f, m, "END_BE_INI_FUNCTION")[1]
        amend_file(f, m, ch_src_pl, num-2, reset_contents)
        f = "frontends/CalcHEP_3_6_27.hpp"
        num = find_string(f, m, "backend_undefs.hpp")[1]
        amend_file(f, m, ch_head, num-2, reset_contents)
        num = find_string(f, m, "BE_FUNCTION")[1]
        amend_file(f, m, "BE_ALLOW_MODELS({0})".format(gum.name), num-2, reset_contents)
    # micrOMEGAs
    if output_opts.mo:
        ver = "3.6.9.2"
        be = "MicrOmegas_" + gum.name
        be_loc = "micromegas/{0}/{1}/libmicromegas.so".format(ver, gum.name)
        f = "frontends/MicrOmegas_{0}_{1}".format(gum.name, ver.replace('.','_'))
        write_file(f+".cpp", m, mo_src, reset_contents)
        write_file(f+".hpp", m, mo_head, reset_contents)
        add_to_backend_locations(be, be_loc, ver, reset_contents)
        add_micromegas_to_cmake(gum.name, reset_contents)
        # TODO add entry to DarkBit_rollcall -> mo capabilities (Xf_micromegas?)
    # Pythia
    if output_opts.pythia:
        be = "pythia_"+gum.name.lower()
        safe_ver = "8_"+base_pythia_version
        ver = "8."+base_pythia_version
        copy_file("default_bossed_versions.hpp", m, output_dir, reset_contents, existing = True)
        copy_file("BOSS/configs/"+be+"_"+safe_ver+".py", m, output_dir, reset_contents, existing = False)
        copy_file(be+"/"+ver+"/patch_"+be+"_"+ver+".dif", m, output_dir, reset_contents, existing = False)
        add_to_backend_locations("Pythia_"+gum.name, be+"/"+ver+"/lib/libpythia8.so", ver, reset_contents)
    # Vevacious
    if output_opts.vev:
        write_vevacious_rollcall(gum.name, gum.spec, reset_contents)        

    # Write a simple YAML file.
    if decaybit: 
        drop_yaml_file(gum.name, parameters, add_higgs, reset_contents)
        print(("GUM has dropped a test YAML file at "
               "$GAMBIT/yaml_files/{0}.yaml!").format(gum.name))

    # Write a config file to make compilation easier
    write_config_file(output_opts, gum.name, reset_contents)

    # Save output to a .mug file for resetting purposes
    mug_file = "mug_files/{0}.mug".format(gum.name)
    drop_mug_file(mug_file, reset_contents)

    print("")
    print("Changes saved to {}".format(mug_file))
    print("If you need to reset GAMBIT, do:")
    print("\t./gum -r {}".format(mug_file))

    # All done!
    print("")
    print("GUM has finished successfully!")
    print("Please (re)compile GAMBIT, by running {}_config.sh".format(gum.name))
    print("")

# If reset is called
elif args.reset:
	revert(args.reset)
	model = os.path.split(args.reset)[-1].strip('.mug')
	print("GUM has removed the model '{0}' from GAMBIT.".format(model))


else:
    raise GumError(("\n\n\tHi! You must be new here. Usage: gum -f inifile.gum"
                    "\n\tOr try gum -h for help."))
