#!/usr/bin/env python

"""
GUM - GAMBIT Universal Models
"""

import sys
import os
if not sys.version_info[0] == 2 or sys.version_info[1] < 6:
    sys.exit('Please use python 2.6 or later -- but not python 3.')

import argparse
import numpy as np

from src import *

from collections import defaultdict

parser = argparse.ArgumentParser(description="From Lagrangian to scans: GUM "
                                             "(GAMBIT Universal Models)")

# Optional command-line arguments
parser.add_argument('-f', '--file', type=str,
                      help="Specify input .GUM file.")
parser.add_argument("-d", "--dryrun", action='store_true',
                    help="GUM will perform a dry run, not saving any output.")
parser.add_argument("-r", "--reset", type=str,
					help="GUM will reset GAMBIT back to a previous version, given an input .mug file generated by GUM.")
# Retrieve command-line arguments
args = parser.parse_args()

# Don't let the user create and reset at the same time
if args.file and args.reset:
	raise GumError(("\n\n\tYou may use only one of the "
					"--file or --reset flags at any one time!"))

if args.reset and args.dryrun:
	raise GumError(("\n\n\tYou have requested a dryrun and a reset together:"
					" this is not supported."))

# Input .gum file.
if args.file:

    if args.dryrun:
        print("**********************************************************")
        print("GUM called with a dry run -- will not be writing to files!")
        print("**********************************************************")

    # Parse the .gum file
    inputs = check_gum_file(args.file)
    gum, output_opts = fill_gum_object(inputs)

    # First, check the required backends are installed
    check_backends(output_opts)

    # Create the output directory for all generated files for this model
    output_dir = os.path.join(os.getcwd(),"Outputs", gum.name)
    mkdir_if_absent(output_dir)

    # Check for a DM candidate
    if gum.dm_pdg:
        darkbit = True
    else:
        darkbit = False
        print(("No DM candidate requested -- no DarkBit routines will "
               "be written.\n"))

    # Check whether we need to generate DecayBit output or not
    if output_opts.ch or output_opts.sph:
        decaybit = True
    else:
        decaybit = False

    # Check whether we need to generate new ColliderBit code or not
    if output_opts.pythia:
        colliderbit = True
    else:
        colliderbit = False


    """
    FEYNRULES/SARAH
    """
    if gum.math == 'feynrules':

        from lib.libfr import *

        # Create the output directory for all generated FeynRules files
        mkdir_if_absent(output_dir + "/FeynRules")

        options = FROptions("feynrules", gum.name, gum.restriction, gum.LTot)
        partlist = FRVectorOfParticles()
        paramlist = FRVectorOfParameters()
        outputs = FROutputs()
        backends = FRBackends()
        backends.extend(x for x in output_opts.bes())

        # Hit it
        all_feynrules(options, partlist, paramlist, outputs, backends)

        # Get a list of all non-SM particles
        fr_bsm = [x for x in partlist if x.SM() is False]

        # Make all BSM particles work with GUM's native Particle class.
        # Find out if the spectrum needs a dependency on StandardModel_Higgs.
        bsm_particle_list, add_higgs = fr_part_to_gum_part(fr_bsm)

        # Initialise all of the model parameters, for writing spectra,
        # model files - remove all SM stuff from parameter list.
        parameters = fr_params(paramlist, add_higgs)

    elif gum.math == 'sarah':

        from lib.libsarah import *

        # Create the output directory for all generated SARAH files
        mkdir_if_absent(output_dir + "/SARAH")

        options = SARAHOptions("sarah", gum.name, '')
        partlist = SARAHVectorOfParticles()
        paramlist = SARAHVectorOfParameters()
        outputs = SARAHOutputs()
        backends = SARAHBackends()
        backends.extend(x for x in output_opts.bes())

        # Hit it
        all_sarah(options, partlist, paramlist, outputs, backends)

        # Get a list of all non-SM particles
        sarah_bsm = [x for x in partlist if x.SM() is False]

        # Make all BSM particles work with GUM's native Particle class.
        bsm_particle_list = sarah_part_to_gum_part(sarah_bsm)


    # Clean up calchep files from SARAH/FeynRules, then copy them to the GAMBIT backendDir
    # TODO make this go to the Outputs dir instead, then copy (or ln -s?) from there to CalcHEP later.
    if output_opts.ch:
        clean_calchep_model_files(outputs.get_ch(), gum.name)

    # Move MadGraph files from SARAH/FeynRules to contrib/MadGraph/models
    # TODO make this go to the Outputs dir instead, then ln -s from there to MadGraph later.
    if output_opts.pythia:
        copy_madgraph_files(outputs.get_mg(), gum.name)

    """
    MADGRAPH
    """
    if output_opts.pythia:  #TODO this should eventually be conditional on a dedicated MadGraph flag, as it will eventually be needed for MadDM and other things than just Pythia.

        # Link MadGraph files output by SARAH/FeynRules to MadGraph's contrib/MadGraph/models
        # still TODO

        # Create the output directory for all generated MadGraph files
        mg5_output_dir = output_dir + "/MadGraph5_aMC"
        mkdir_if_absent(mg5_output_dir)

        # Clear and remake the copy of Pythia that will receive the matrix element code produced by MadGraph
        pristine_pythia_dir = os.path.join(os.getcwd(),'contrib','pythia')
        new_pythia_dir = mg5_output_dir + "/Pythia_patched"
        remove_tree_quietly(new_pythia_dir)
        copy_tree(pristine_pythia_dir, new_pythia_dir)

        # Create the MadGraph script
        # TODO determine all possible BSM processes automatically when collider_processes is missing from the .gum file
        make_madgraph_script(mg5_output_dir, gum.name, output_opts.collider_processes)

        # Add MadGraph to path and import the python interface
        mg5_dir = os.path.join(os.getcwd(),'contrib','MadGraph')
        sys.path.append(mg5_dir)
        import madgraph.interface.master_interface as mi

        # Run MadGraph
        call_madgraph(mg5_dir, mg5_output_dir,mi)


    print("")
    print("Finished running external codes.  Now writing proposed GAMBIT code.")


    """
    REGISTRATION OF NEW PARTICLES
    """

    # Initialise DM particle from particle list
    if darkbit:
        for i in xrange(len(partlist)):
            part = partlist[i]
            # If specified, initialise the DM candidate
            if part.pdg() == gum.dm_pdg:
                dm = Particle(part.name(), part.antiname(),
                              part.spinX2(), part.pdg(),
                              part.mass())

    # If we haven't found the DM candidate in the particle list...
    if darkbit and not dm:
        raise GumError(("\n\nThe DM candidate specified has not been found "
                        "in the {0} file. Please check your .gum "
                        "file as well as the {0} file.").format(gum.math))

    print("Adding new model {0} to GAMBIT.").format(gum.name)

    # Need to know how GAMBIT refers to each particle. Scrape information
    # from particle_database.yaml.
    gambit_pdgs, decaybit_dict = get_gambit_particle_pdg_dict()

    # Check every new particle is in the database
    missing_parts = check_all_particles_present(partlist, gambit_pdgs)

    if len(missing_parts) != 0:
        if args.dryrun:
            print("GUM won't add to the particle database for a dryrun.")
            print("GUM will fail shortly. That's a promise.")
        else:
            # Add new particles to the database.
            add_new_particleDB_entry(missing_parts, gum.dm_pdg)
            # Mow reload the dictionaries
            gambit_pdgs, decaybit_dict = get_gambit_particle_pdg_dict()

    # Once we've got the GAMBIT dictionaries, we can add the new parameters
    # as pole masses to the

    # Add all of the masses of BSM particles as pole masses.
    # I think this is okay..? TODO: check this is okay.
    for i in xrange(len(bsm_particle_list)):
        p = bsm_particle_list[i]
        x = SpectrumParameter(pdg_to_particle(p.PDG_code, gambit_pdgs),
                              "Pole_Mass", gb_input=p.mass)
        parameters.append(x)

    """
    MODELS
    """

    model_header = add_to_model_hierarchy(gum.spec, gum.name, parameters)
    spec_contents = write_spectrumcontents(gum.name, parameters)
    subspec_wrapper = write_subspectrum_wrapper(gum.name, parameters)
    reg_spec, reg_spec_num = add_to_registered_spectra(gum.name)

    """
    VERTICES
    """

    # Conditional on CalcHEP files being made.
    if output_opts.ch:
        # Obtain all model interactions from CalcHEP model files.
        interactions, calchep_pdg_codes = get_vertices(outputs.get_ch())

        # Obtain all 3-point vertices that are BSM.
        three_pi_bsm = [interaction.particles for interaction in interactions
                        if interaction.num_particles() == 3
                        and interaction.is_sm() == False]

        three_body_decays = decay_sorter(three_pi_bsm)
    else:
        calchep_pdg_codes = None

    """
    DECAYBIT
    """

    if decaybit:

        print("Writing new module functions for DecayBit")

        # CalcHEP specific decays
        if output_opts.ch:
            decaybit_src_ch = ""

            # Pass all interactions by first PDG code needed.
            for i in xrange(len(three_body_decays)):
                decaybit_src_ch += write_decaytable_entry(three_body_decays[i],
                                                          gum.name,
                                                          calchep_pdg_codes,
                                                          gambit_pdgs,
                                                          decaybit_dict)

            decay_roll, new_decays = \
                write_decaybit_rollcall_entry(gum.name, gum.spec,
                                              three_body_decays,
                                              decaybit_dict, gambit_pdgs)

            all_decays_src, all_decays_header = \
                amend_all_decays(gum.name, gum.spec, new_decays)

        # TODO - decays from SPheno

    """
    COLLIDERBIT
    """
    if colliderbit:

        print("Writing new module functions for ColliderBit")

        # Create the output directory for all generated ColliderBit sources
        cb_output_dir = output_dir + "/ColliderBit"
        mkdir_if_absent(cb_output_dir)

        # Write ColliderBit headers and sources for the new model
        new_colliderbit_model(cb_output_dir, gum.name)

    """
    DARKBIT
    """
    if darkbit:

        pc = None
        sv = None
        dd = None
        mo = None
        mdm = None
        ann_products = None
        propagators = None

        print("Writing new module functions for DarkBit")

        # Write process catalogue output
        pc = True

        # Conditional on CalcHEP files being made.
        if output_opts.ch:

          sv = True

          # Obtain all three-field vertices
          three_f = [interaction.particles for interaction in interactions
                     if interaction.num_particles() == 3]
          # And all 4-point BSM vertices
          four_f = [interaction.particles for interaction in interactions
                    if interaction.num_particles() == 4
                    and interaction.is_sm() == False]

          # Get all annihilation products and propagators involved in
          # DM + DM -> X + Y at tree-level
          ann_products, propagators = sort_annihilations(dm, three_f, four_f)

        # If micrOMEGAs requested
        if output_opts.mo:

            print("  Writing micrOMEGAs interface for DarkBit.")
            # TODO: write simple frontend for MO.

        """
        # If MadDM requested
        if output_opts.mdm:

            print("Writing MadDM interface for DarkBit.")
            # TODO: backend MadDM...

        # TODO - pass over required information for direct detection
        # DirectDM?
        if output_opts.ddm:

            print("Writing direct detection interface for DarkBit.")

            # Write direct detection interface
            dd = True
        """

        darkbit_src = write_darkbit_src(dm, pc, sv, dd, ann_products,
                                        propagators, gambit_pdgs, gum.name,
                                        calchep_pdg_codes, bsm_particle_list)

        pc_cap, dd_cap, dmid_cap = write_darkbit_rollcall(gum.name, pc, dd)

    """
    SPECBIT
    """

    # Determine when "simple_SMinputs" needed from FR/S model file
    spectrum_src = write_basic_spectrum(gum.name, parameters, gum.spec, add_higgs)
    spectrum_header = write_spectrum_header(gum.name, add_higgs)
    spec_rollcall = write_specbit_rollcall(gum.name)

    """
    BACKENDS
    """

    print("Writing new frontends and patches for backend codes.")

    # Conditional on CalcHEP files being made.
    if output_opts.ch: # If no parent
        ch_src_sl, ch_src_pl, ch_head = add_calchep_switch(gum.name, gum.spec)

    # Have we created a new Pythia backend?
    if output_opts.pythia:
        print("  Patching Pythia to inject new matrix elements into its Process Container and shared library.")
        fix_pythia_lib(gum.name, new_pythia_dir)
        print("  Creating a diff vs original version of Pythia.")
        write_backend_patch(output_dir, pristine_pythia_dir, new_pythia_dir, "pythia_"+gum.name.lower(), "8."+base_pythia_version)
        print("  Creating a BOSS config file for Pythia_"+gum.name+".")
        write_boss_config_for_pythia(gum.name, output_dir)
        print("  Creating a cmake entry for Pythia"+gum.name+".")
        add_new_pythia_to_backends_cmake(gum.name, output_dir)
        print("  Setting the default version of Pythia_"+gum.name+" for BOSSed classes to 8."+base_pythia_version)
        write_new_default_bossed_version("Pythia_"+gum.name, "8."+base_pythia_version, output_dir)

    # Stop now if we're just doing a dry run
    if args.dryrun:
        print("")
        print("Dry run finished.")
        print("")
        exit()

    print("")
    print("Now putting the new code into GAMBIT.")

    #print("model_header")
    #print(model_header)
    #print("spec_contents")
    #print(spec_contents)
    #print("subspec_wrapper")
    #print(subspec_wrapper)
    #print("spectrum_src")
    #print(spectrum_src)
    #print("spec_contents")
    #print(spec_contents)
    #print("spectrum_header")
    #print(spectrum_header)
    #print("decaybit_src_ch")
    #print(decaybit_src_ch)
    #print("decay_roll")
    #for i in xrange(len(decay_roll)):
    #    print(decay_roll[i][0])
    #    print(decay_roll[i][1])

    # All file writing routines HERE, once everything has gone okay.

    # Create defaultdict for reset file.
    # There are 2 nodes: new_files and amended_files.
    reset_contents = defaultdict(lambda: defaultdict(list))

    # Models
    m = "Models"
    write_file("models/" + gum.name + ".hpp", m, model_header, reset_contents)
    write_file("SpectrumContents/" + gum.name + ".cpp", m, spec_contents, reset_contents)
    write_file("SimpleSpectra/" + gum.name + "SimpleSpec" + ".hpp", m, subspec_wrapper, reset_contents)
    amend_file("SpectrumContents/RegisteredSpectra.hpp", m, reg_spec, reg_spec_num, reset_contents)

    # SpecBit
    m = "SpecBit"
    write_file("SpecBit_" + gum.name + ".cpp", m, spectrum_src, reset_contents)
    write_file("SpecBit_" + gum.name + "_rollcall.hpp", m, spectrum_header, reset_contents)
    num = find_string("SpecBit_rollcall.hpp", m, "SpecBit_tests_rollcall.hpp")[1]
    amend_file("SpecBit_rollcall.hpp", m, spec_rollcall, num, reset_contents)

    # DecayBit
    if decaybit:
        m = "DecayBit"
        # Append to the end of DecayBit -- just before all_decays
        num = find_string("DecayBit.cpp", m, "void all_decays")[1]
        if output_opts.ch:
            amend_file("DecayBit.cpp", m, decaybit_src_ch, num-4, reset_contents)
            for i in xrange(len(decay_roll)):
                if find_capability(decay_roll[i][0], m)[0]:
                    amend_rollcall(decay_roll[i][0], m, decay_roll[i][1], reset_contents)
                else:
                    num = find_string("DecayBit_rollcall.hpp", m, "#define CAPABILITY decay_rates")[1]
                    amend_file("DecayBit_rollcall.hpp", m, decay_roll[i][1], num-2, reset_contents)
            if len(new_decays) > 0:
                num = find_string("DecayBit_rollcall.hpp", m, "DEPENDENCY(omega_decay_rates")[1]
                amend_file("DecayBit_rollcall.hpp", m, all_decays_header, num, reset_contents)
                num = find_string("DecayBit.cpp", m, "decays(\"omega\")")[1]
                amend_file("DecayBit.cpp", m, all_decays_src, num, reset_contents)

    # DarkBit
    if darkbit:
        m = "DarkBit"
        write_file(gum.name + ".cpp", m, darkbit_src, reset_contents)
        if pc:
            amend_rollcall("TH_ProcessCatalog", m, pc_cap, reset_contents)
        if dd:
            amend_rollcall("DD_couplings", m, dd_cap, reset_contents)
        amend_rollcall("DarkMatter_ID", m, dmid_cap, reset_contents)

    # ColliderBit
    m = "ColliderBit"
    if output_opts.pythia:
        copy_file("models/"+gum.name+".hpp", m, output_dir, reset_contents, existing = False)
        copy_file("models/"+gum.name+".cpp", m, output_dir, reset_contents, existing = False)

    # Backends
    m = "Backends"
    # cmake
    if output_opts.pythia:
        copy_file("backends.cmake", "cmake", output_dir, reset_contents, existing = True)
    # CalcHEP
    if output_opts.ch:
        f = "frontends/CalcHEP_3_6_27.cpp"
        num = find_string(f, m, "setModel(modeltoset, 1)")[1]
        amend_file(f, m, ch_src_sl, num-2, reset_contents)
        num = find_string(f, m, "END_BE_INI_FUNCTION")[1]
        amend_file(f, m, ch_src_pl, num-2, reset_contents)
        f = "frontends/CalcHEP_3_6_27.hpp"
        num = find_string(f, m, "backend_undefs.hpp")[1]
        amend_file(f, m, ch_head, num-2, reset_contents)
        num = find_string(f, m, "BE_FUNCTION")[1]
        amend_file(f, m, "BE_ALLOW_MODELS({0})".format(gum.name), num-2, reset_contents)
    # micrOMEGAs
    if output_opts.mo:
        ver = "3.6.9.2"
        be = "MicrOmegas_" + gum.name
        be_loc = "micromegas/{0}/{1}/libmicromegas.so".format(ver, gum.name)
        f = "frontends/MicrOmegas_{0}_{1}".format(gum.name, ver.replace('.','_'))
        write_file(f+".cpp", m, mo_src, reset_contents)
        write_file(f+".hpp", m, mo_head, reset_contents)
        add_to_backend_locations(be, be_loc, ver, reset_contents)
    # Pythia
    if output_opts.pythia:
        be = "pythia_"+gum.name.lower()
        safe_ver = "8_"+base_pythia_version
        ver = "8."+base_pythia_version
        copy_file("default_bossed_versions.hpp", m, output_dir, reset_contents, existing = True)
        copy_file("BOSS/configs/"+be+"_"+safe_ver+".py", m, output_dir, reset_contents, existing = False)
        copy_file(be+"/"+ver+"/patch_"+be+"_"+ver+".dif", m, output_dir, reset_contents, existing = False)
        add_to_backend_locations("Pythia_"+gum.name, be+ver+"/lib/libpythia8.so", ver, reset_contents)
    # MadDM

    # Save output to a .mug file for resetting purposes
    mug_file = "mug_files/{0}.mug".format(gum.name)
    drop_mug_file(mug_file, reset_contents)

    print("")
    print("Changes saved to {}".format(mug_file))
    print("If you need to reset GAMBIT, do:")
    print("\t./gum -r {}".format(mug_file))

    # All done!
    print("")
    print("GUM has finished successfully!")
    print("Please (re)compile GAMBIT.")
    print("")

# If reset is called
elif args.reset:
	revert(args.reset)
	model = os.path.split(args.reset)[-1].strip('.mug')
	print("GUM has removed the model '{0}' from GAMBIT.".format(model))


else:
    raise GumError(("\n\n\tHi! You must be new here. Usage: gum -f inifile.gum"
                    "\n\tOr try gum -h for help."))
