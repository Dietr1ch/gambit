#!/usr/bin/env python
#
#  GUM: GAMBIT Universal Models
#  ****************************
#  \file
#
#  Main GUM script
#
#  *************************************
#
#  \author Sanjay Bloor
#          (sanjay.bloor12@imperial.ac.uk)
#  \date 2017, 2018, 2019, 2020... 
#
#  \author Tomas Gonzalo
#          (tomas.gonzalo@monash.edu)
#  \date 2019, 2020
#
#  **************************************


import sys
import os
if not sys.version_info[0] == 2 or sys.version_info[1] < 6:
    sys.exit('Please use python 2.6 or later -- but not python 3.')

import argparse
import numpy as np

from src import *

from collections import defaultdict
from distutils.dir_util import copy_tree

parser = argparse.ArgumentParser(description="From Lagrangian to scans: GUM "
                                             "(GAMBIT Universal Models)")

# Optional command-line arguments
parser.add_argument('-f', '--file', type=str,
                      help="Specify input .GUM file.")
parser.add_argument("-d", "--dryrun", action='store_true',
                    help="GUM will perform a dry run, not saving any output.")
parser.add_argument("-r", "--reset", type=str,
                    help=("GUM will reset GAMBIT back to a previous version, "
                         "given an input .mug file generated by GUM."))
# Retrieve command-line arguments
args = parser.parse_args()

# Don't let the user create and reset at the same time
if args.file and args.reset:
    raise GumError(("\n\n\tYou may use only one of the "
                    "--file or --reset flags at any one time!"))

if args.reset and args.dryrun:
    raise GumError(("\n\n\tYou have requested a dryrun and a reset together:"
                    " this is not supported."))

# Input .gum file.
if args.file:

    if args.dryrun:
        print("**********************************************************")
        print("GUM called with a dry run -- will not be writing to files!")
        print("**********************************************************")

    try:
        # Parse the .gum file
        inputs = check_gum_file(args.file)
        gum, output_opts = fill_gum_object(inputs)

        # SARAH removes hyphens and underscores for .f90 files...
        clean_model_name = gum.name.replace('-','').replace('_','')

        # Create the output directory for all generated files for this model
        output_dir = os.path.join(os.getcwd(),"Outputs", gum.name)
        mkdir_if_absent(output_dir)

        # Check for a DM candidate
        if gum.dm_pdg:
            darkbit = True
        else:
            darkbit = False
            print(("No DM candidate requested -- no DarkBit routines will "
                   "be written.\n"))

        # Check whether we need to generate DecayBit output or not
        # N.B. if we don't something is quiiiiite likely to go wrong
        if output_opts.ch or output_opts.spheno:
            decaybit = True
        else:
            decaybit = False

        # Check whether we need to generate new ColliderBit code or not
        # Also check whether the correct YAML entries are filled
        if output_opts.pythia:
            colliderbit = True
            if not 'pythia' in output_opts.options:
                raise GumError(("\n\nPythia output requested but no "
                                "collider_processes specified.\n"
                                "Please amend your .gum file!"))
            collider_processes = output_opts.options['pythia'].get('collider_processes')
            if collider_processes == None:
                raise GumError(("\n\nPythia output requested but no "
                                "collider_processes specified.\n"
                                "Please amend your .gum file!"))
        else:
            colliderbit = False

        # Check to see if any of the proposed new files will
        # clash with any existing files.
        check_for_existing_entries(gum.name, darkbit, colliderbit, output_opts)

        """
        FEYNRULES/SARAH
        """
        if gum.math == 'feynrules':

            # Let GUM do a quick and dirty parse of the file first, before we
            # fire up a Mathematica kernel
            parse_feynrules_model_file(gum.mathname, gum.base_model, 
                                       output_opts)

            from lib.libfr import *

            # Create the output directory for all generated FeynRules files
            mkdir_if_absent(output_dir + "/FeynRules")

            options = FROptions("feynrules", gum.mathname, gum.base_model,
                                gum.restriction, gum.LTot)
            partlist = FRVectorOfParticles()
            paramlist = FRVectorOfParameters()
            outputs = FROutputs()
            backends = FRBackends()
            backends.extend(x for x in output_opts.bes())
            mixings = {}
            err = FRError()

            # Hit it
            all_feynrules(options, partlist, paramlist, outputs, backends, err)

            if err.is_error():
                raise GumError( err.what() )

            # Get a list of all non-SM particles
            fr_bsm = [x for x in partlist if x.SM() is False]

            # Make all BSM particles work with GUM's native Particle class.
            # Find out if the spectrum needs a dependency on StandardModel_Higgs
            bsm_particle_list, add_higgs = fr_part_to_gum_part(fr_bsm)

        elif gum.math == 'sarah':

            # Let GUM do a quick and dirty parse of the file first, before we
            # fire up a Mathematica kernel
            parse_sarah_model_file(gum.mathname, output_opts)

            from lib.libsarah import *

            # Create the output directory for all generated SARAH files
            mkdir_if_absent(output_dir + "/SARAH")

            options = SARAHOptions("sarah", gum.mathname)
            options.setOptions(**output_opts.options)
            partlist = SARAHVectorOfParticles()
            paramlist = SARAHVectorOfParameters()
            flags = SARAHMapStrBool()
            outputs = SARAHOutputs()
            bcs = SARAHMapStrStr()
            mixings = SARAHMapStrStr()
            sphenodeps = SARAHVectorOfParameters()
            backends = SARAHBackends()
            backends.extend(x for x in output_opts.bes())
            err = SARAHError()

            # Hit it
            all_sarah(options, partlist, paramlist, outputs, backends, flags, 
                      mixings, bcs, sphenodeps, err)

            if err.is_error():
                raise GumError( err.what() )

            # Get a list of all non-SM particles
            sarah_bsm = [x for x in partlist if x.SM() is False]

            # Make all BSM particles work with GUM's native Particle class.
            bsm_particle_list, add_higgs = sarah_part_to_gum_part(sarah_bsm)

        print("Finished extracting parameters from " + gum.math + ".")

        # Check to see if the DM particle exists in the model file.
        # First, initialise DM particle from particle list
        if darkbit:
            dm_set = False
            for i in xrange(len(partlist)):
                part = partlist[i]
                # If specified, initialise the DM candidate
                if part.pdg() == gum.dm_pdg:
                    dm = Particle(part.name(), part.antiname(),
                                  part.spinX2(), part.pdg(),
                                  part.mass())
                    dm_set = True

            # If we haven't found the DM candidate in the particle list, 
            # throw an error
            if not dm_set:
                raise GumError(("\n\nThe WIMP candidate specified has not been "
                                "found in the {0} file. Please check your .gum "
                                "file and {0} file are consistent.\n"
                                "PDG code given = {1}"
                                ).format(gum.math, gum.dm_pdg))

            # If the user has requested MicrOMEGAs output and the DM candidate
            # does not begin with a tilde, then complain (and its anti-p.)
            if output_opts.mo:
                if not dm.name.startswith('~'): 
                    raise GumError(("You have requested MicrOMEGAs output, but "
                                    " your DM candidate has to begin with a '~'"
                                    " for MicrOMEGAs to recognise it as an odd "
                                    "particle.\n Please change the DM name in "
                                    "your {} file!"
                                    ).format(gum.math))                        
                if not dm.antiname.startswith('~'):
                    raise GumError(("You have requested MicrOMEGAs output, but "
                                    " your DM candidate's antiparticle also has"
                                    " to begin with a '~' for MicrOMEGAs to "
                                    "recognise it as an odd particle.\n "
                                    "Note that the default behaviour will be "
                                    "to just add a tilde at the end, and strip "
                                    "any leading tilde, so add a line like:\n"
                                    "\t    AntiParticleName -> \"~Chi\"\n "
                                    "to your {} file!"
                                    ).format(gum.math))           


        """ 
        UFO2MDL

        If we have both UFO and MDL files, run the files through ufo2mdl to 
        compare outputs. This will, e.g., add 4-fermion interactions to CalcHEP
        where FeynRules fails to do so.
        """
        # if output_opts.ch and output_opts.ufo:
        #     print("Calling UFO2MDL.")
        #     blockPrint()  # Stop the UFO2MDL output - there's lots of it
        #     ch_location = compare_mg_and_ch(outputs.get_mg(), outputs.get_ch())
        #     enablePrint() # Bring it back.
        #     if ch_location != outputs.get_ch(): 
        #         print("Updating location of CalcHEP files in GUM...")
        #         outputs.set_ch(ch_location)

        # Move MadGraph files from SARAH/FeynRules to contrib/MadGraph/models
        # TODO make this go to the Outputs dir instead, then ln -s from there 
        # to MadGraph later.
        if output_opts.ufo:
            copy_madgraph_files(outputs.get_mg(), gum.name)

        # Clean up calchep files from SARAH/FeynRules, then copy them to the 
        # GAMBIT backendDir
        # TODO make this go to the Outputs dir instead, then copy (or ln -s?) 
        # from there to CalcHEP later.
        if output_opts.ch or output_opts.mo:
            clean_calchep_model_files(outputs.get_ch(), gum.name)

        """
        MADGRAPH -- Pythia inside here (and MadDM in the future)
        """
        if output_opts.ufo:  
            
            # Pythia
            if output_opts.pythia:  

                collider_processes = output_opts.options['pythia'].get('collider_processes')
                multiparticles = output_opts.options['pythia'].get('multiparticles')
                if collider_processes is not None:
                    # Link MadGraph files output by SARAH/FeynRules to 
                    # MadGraph's contrib/MadGraph/models
                    # still TODO

                    # Create the output directory for all generated MadGraph 
                    # files
                    mg5_output_dir = output_dir + "/MadGraph5_aMC"
                    mkdir_if_absent(mg5_output_dir)

                    # Clear and remake the copy of Pythia that will receive the
                    #  matrix element code produced by MadGraph
                    pristine_pythia_dir = os.path.join(os.getcwd(),
                                                       'contrib','pythia')
                    new_pythia_dir = mg5_output_dir + "/Pythia_patched"
                    remove_tree_quietly(new_pythia_dir)
                    copy_tree(pristine_pythia_dir, new_pythia_dir)

                    # Create the MadGraph script
                    # TODO determine all possible BSM processes automatically 
                    # when collider_processes is missing from the .gum file
                    make_madgraph_script(mg5_output_dir, gum.name, 
                                         collider_processes, 
                                         multiparticles)

                    # Add MadGraph to path and import the python interface
                    mg5_dir = os.path.join(os.getcwd(),'contrib','MadGraph')
                    sys.path.append(mg5_dir)
                    import madgraph.interface.master_interface as mi

                    # Run MadGraph
                    call_madgraph(mg5_dir, mg5_output_dir, mi)

                else:
                    raise GumError(("Pythia output requested but no "
                                    "collider_processes specified.\n"
                                    "Please amend your .gum file!"))

        print("")
        print("Finished running external codes...")
        print("Now attempting to write proposed GAMBIT code.")

        # Create defaultdict for reset file.
        # There are 2 nodes: new_files and amended_files.
        # todo: add backend patches (e.g. pythia stuff) to reset_contents
        # so ordering is not important.
        reset_contents = defaultdict(lambda: defaultdict(list))

        # Dictionaries with capability and model definitions to add to the list
        capability_definitions = {}
        model_definitions = {}

        """
        REGISTRATION OF NEW PARTICLES
        """

        # Need to know how GAMBIT refers to each particle. Scrape information
        # from particle_database.yaml.
        gambit_pdgs, decaybit_dict = get_gambit_particle_pdg_dict()

        # Check every new particle is in the database
        missing_parts = check_all_particles_present(partlist, gambit_pdgs)

        if len(missing_parts) != 0:
            if args.dryrun:
                print("GUM won't add to the particle database for a dryrun.")
                print("GUM will fail shortly. That's a promise.")
            else:
                # Add new particles to the database, refresh the dicts
                gambit_pdgs, decaybit_dict = \
                    add_new_particleDB_entry(missing_parts, gum.dm_pdg, 
                                             gambit_pdgs, decaybit_dict,
                                             reset_contents, gum.name)

        # Grab the antiparticles
        antiparticle_dict = get_antiparticles(partlist)

        # Return a list of the PDG codes of all Higgses involved
        higgses, neutral_higgses, charged_higgses = \
            get_higgses(bsm_particle_list)

        """
        PARAMETERS
        """

        # Initialise all of the model parameters, for writing spectra,
        # model files - remove all SM stuff from parameter list.
        if gum.math == "feynrules":
            parameters = fr_params(paramlist, add_higgs)
        else:
            parameters = sarah_params(paramlist, mixings, add_higgs, 
                                      gambit_pdgs, partlist, bcs)

        # Get the parameters sorted by block
        blockparams = sort_params_by_block(parameters, mixings)
        # Dict of which parameters are real.
        reality_dict = parameter_reality_dict(parameters)

        """
        MODELS
        """
        
        # TODO: should the BSM particle masses be model parameters?
        # S.B. 4/10
        # For now I've made this for FeynRules only, since the masses 
        # aren't generated by spectra. For SARAH however we only want to use 
        # the fundamental Lagrangian parameters. 

        print("Adding new model {0} to GAMBIT.").format(gum.name)
        
        # For FeynRules, use masses of particles as input parameters
        if gum.math == 'feynrules':
            # Add all of the masses of BSM particles as pole masses. 
            # Also removes all duplicate entries that can arise
            # for e.g. multiplets with same tree-level masses
            parameters = add_masses_to_params(parameters, bsm_particle_list, 
                                              gambit_pdgs, add_higgs)
        
        # Get the model parameters out of the parameter list
        model_parameters = get_model_parameters(parameters, add_higgs)

        model_header = add_to_model_hierarchy(gum.spec, gum.name, 
                                              model_parameters, model_definitions,
                                              capability_definitions)

        # Get the spectrum parameters out of the parameter list
        spectrum_parameters = get_spectrum_parameters(parameters, blockparams,
                                                      bsm_particle_list, 
                                                      partlist, gambit_pdgs,
                                                      output_opts.spheno)

        subspec_wrapper = write_subspectrum_wrapper(gum.name, 
                                                    spectrum_parameters)
        spec_contents = write_spectrumcontents(gum.name, spectrum_parameters)
        reg_spec, reg_spec_num = add_to_registered_spectra(gum.name)

        """
        SPHENO
        """

        # Forward declare some variables
        spheno_decays = {}

        if output_opts.spheno:

            # Move SPheno files from SARAH to Outputs.
            copy_spheno_files(clean_model_name, output_dir, SPHENO_DIR, 
                              outputs.get_sph())

            # Pristine and patched SPhenos
            pristine_spheno_dir = output_dir + "/SPheno"
            patched_spheno_dir = output_dir + "/SPheno_patched"

            # Patch the files
            print("Patching SPheno...")

            patch_spheno(clean_model_name, patched_spheno_dir, flags, 
                         bsm_particle_list)

            fullpath = "Backends/patches/sarah-spheno/{0}/{1}".format(
                                                               SPHENO_VERSION,
                                                               gum.name)
            fullfile = "patch_sarah-spheno_"+SPHENO_VERSION+"_"+gum.name
            # Create a diff vs. the out of the [SARAH]box
            write_backend_patch(output_dir, pristine_spheno_dir, 
                                patched_spheno_dir, 
                                "sarah-spheno",
                                SPHENO_VERSION, 
                                fullpath = fullpath,
                                fullfile = fullfile)



            # Get the defs of any SPheno dependencies
            deps = spheno_dependencies(sphenodeps)

            # Write frontends for SPheno. That's a lotta scrapin'.
            print("Writing SPheno frontends.")
            spheno_src, spheno_header, backend_types, btnum = \
                write_spheno_frontends(clean_model_name, parameters, 
                                       bsm_particle_list, flags, 
                                       patched_spheno_dir, output_dir, 
                                       blockparams, gambit_pdgs, mixings, 
                                       reality_dict, deps, bcs,
                                       charged_higgses, neutral_higgses, 
                                       gum.name, capability_definitions)

            # cmake entry
            spheno_cmake = write_spheno_cmake_entry(gum.name, SPHENO_VERSION,
                                                    clean_model_name)

            print("Writing SPheno decay table.")
            spheno_decay_tables, spheno_decays = \
                make_spheno_decay_tables(patched_spheno_dir, clean_model_name)

            # DecayBit entry
            print("Writing SPheno module functions for DecayBit.")
            spheno_decay_src, spheno_decay_header = \
                write_spheno_decay_entry(gum.name, clean_model_name)

            # HiggsCouplingsTable
            hct_src, hct_head, hb_pattern = new_hct_switch(gum.name, gum.spec, 
                                                           neutral_higgses,
                                                           gambit_pdgs,
                                                           len(higgses))


        """
        CALCHEP
        """

        # Obtain all model interactions from CalcHEP model files.
        # Also grab the PDG codes of each particle and the names of their 
        # masses -- we'll want to use this for writing the micromegas frontend
        calchep_pdg_codes = {}
        if output_opts.ch or output_opts.mo:
            
            (interactions, calchep_pdg_codes, calchep_masses,
            calchep_widths, aux_particles) = get_vertices(outputs.get_ch())

        if output_opts.ch:

            # Obtain all 3-point vertices that are BSM.
            three_pi_bsm = [i.particles for i in interactions
                            if i.num_particles() == 3 and i.is_sm() == False]

            three_body_decays = decay_sorter(three_pi_bsm, aux_particles, 
                                             antiparticle_dict)

            print("Writing CalcHEP module functions for DecayBit")
            # CalcHEP specific decays
            decaybit_src_ch = ""

            """
            DECAYBIT
            """
            # TODO: if we have SPheno output, do we /really/ want 
            # to provide CalcHEP DecayBit entries too? 

            # Pass all interactions by first PDG code needed.
            for i in xrange(len(three_body_decays)):
                decaybit_src_ch += write_decaytable_entry_calchep(
                                                          three_body_decays[i],
                                                          gum.name,
                                                          calchep_pdg_codes,
                                                          gambit_pdgs,
                                                          decaybit_dict)

            decay_roll, new_decays = \
                write_decaybit_rollcall_entry_calchep(gum.name, gum.spec,
                                                      three_body_decays,
                                                      decaybit_dict, 
                                                      gambit_pdgs,
                                                      capability_definitions)

            all_decays_src, all_decays_header = \
                amend_all_decays_calchep(gum.name, gum.spec, new_decays)

            # Entries for header file
            ch_src_sl, ch_src_pl, ch_head = add_calchep_switch(gum.name, 
                                                               gum.spec)

        """
        COLLIDERBIT
        """
        if colliderbit:

            print("Writing new module functions for ColliderBit")

            # Create the output directory for all generated ColliderBit sources
            cb_output_dir = output_dir + "/ColliderBit"
            mkdir_if_absent(cb_output_dir)

            # Write ColliderBit headers and sources for the new model
            new_colliderbit_model(cb_output_dir, gum.name)

        """
        DARKBIT
        """

        if darkbit:

            pc = None
            sv = None
            mo = None
            mdm = None
            ann_products = None
            propagators = None

            print("Writing new module functions for DarkBit")


            # Conditional on CalcHEP files being made.
            if output_opts.ch:

                sv = True
                # Write process catalogue output
                pc = True

                # Obtain all three-field vertices
                three_f = [i.particles for i in interactions
                           if i.num_particles() == 3]
                # And all 4-point BSM vertices
                four_f = [i.particles for i in interactions
                         if i.num_particles() == 4 and i.is_sm() == False]

                # Get all annihilation products and propagators involved in
                # DM + DM -> X + Y at tree-level

                ann_products, propagators = \
                    sort_annihilations(dm, three_f, four_f, aux_particles,
                                       antiparticle_dict)

            # If micrOMEGAs requested
            if output_opts.mo:

                print("Writing micrOMEGAs interface for DarkBit.")

                mo_src = write_micromegas_src(gum.name, gum.spec, gum.math, 
                                              parameters, bsm_particle_list, 
                                              gambit_pdgs, calchep_masses, 
                                              calchep_widths)
                mo_head = write_micromegas_header(gum.name, gum.math, 
                                                  parameters)

            darkbit_src = write_darkbit_src(dm, pc, sv, ann_products,
                                            propagators, gambit_pdgs, gum.name,
                                            calchep_pdg_codes, 
                                            bsm_particle_list, higgses)

            pc_cap, dmid_cap, dmconj_cap = write_darkbit_rollcall(gum.name, pc)


        """
        SPECBIT
        """
            
        print("Writing basic container SpecBit interface...")
        spectrum_src = write_spectrum(gum.name, parameters, gum.spec, add_higgs, 
                                      output_opts.spheno, gambit_pdgs,
                                      neutral_higgses, charged_higgses, 
                                      blockparams, bsm_particle_list,
                                      spheno_decays, partlist)

        spectrum_header = write_spectrum_header(gum.name, add_higgs, 
                                                output_opts.spheno, higgses,
                                                capability_definitions)
        spec_rollcall = write_specbit_rollcall(gum.name)

        """
        VEVACIOUS
        """

        if output_opts.vev:

            # Currently require SPheno output for vevacious since we need MINPAR
            # etc. 
            if not output_opts.spheno:
                raise GumError(("Currently gum needs SPheno output to be able "
                                "to produce Vevacious output. Please change "
                                "your .gum file."))
            vev_src = write_vevacious_src(gum.name, outputs.get_vev(), gum.spec, 
                                          blockparams)

        """
        BACKENDS
        """

        print("Writing new frontends and patches for backend codes.")

        # Stop now if we're just doing a dry run
        if args.dryrun:
            print("")
            print("Dry run finished.")
            print("")
            exit()
            
        # Have we created a new Pythia backend?
        if output_opts.pythia:
            print(("Patching Pythia to inject new matrix elements into its "
                   "Process Container and shared library."))
            pythia_groups = output_opts.options['pythia'].get('pythia_groups')
            fix_pythia_lib(gum.name, new_pythia_dir, pythia_groups)
            print("Creating a diff vs original version of Pythia.")
            write_backend_patch(output_dir, pristine_pythia_dir, new_pythia_dir,
                                "pythia_"+gum.name.lower(), 
                                "8."+base_pythia_version)
            print("Writing an additional patch for the new version of Pythia.")
            patch_pythia_patch(parameters, gum.name, reset_contents)
            print("Creating a BOSS config file for Pythia_"+gum.name+".")
            write_boss_config_for_pythia(gum.name, output_dir)
            print("Creating a cmake entry for Pythia"+gum.name+".")
            add_new_pythia_to_backends_cmake(gum.name, output_dir)
            print(("Setting the default version of Pythia_"+gum.name+" for "
                   "BOSSed classes to 8."+base_pythia_version))
            write_new_default_bossed_version("Pythia_"+gum.name, 
                                             "8."+base_pythia_version, 
                                             output_dir)
    
        print("")
        print("Now putting the new code into GAMBIT.")

        # All file writing routines HERE, once everything has gone okay.

        # Models
        m = "Models"
        write_file("models/" + gum.name + ".hpp", m, model_header, 
                   reset_contents)
        write_file("SpectrumContents/" + gum.name + ".cpp", m, spec_contents, 
                   reset_contents)
        write_file("SimpleSpectra/" + gum.name + "SimpleSpec" + ".hpp", m, 
                    subspec_wrapper, reset_contents)
        amend_file("SpectrumContents/RegisteredSpectra.hpp", m, reg_spec, 
                    reg_spec_num, reset_contents)


        # SpecBit
        m = "SpecBit"
        write_file("SpecBit_" + gum.name + ".cpp", m, spectrum_src, 
                   reset_contents)
        write_file("SpecBit_" + gum.name + "_rollcall.hpp", m, spectrum_header, 
                    reset_contents)
        num = find_string("SpecBit_rollcall.hpp", m, 
                          "SpecBit_tests_rollcall.hpp")[1]
        amend_file("SpecBit_rollcall.hpp", m, spec_rollcall, num, 
                   reset_contents)

        # DecayBit
        if decaybit:
            m = "DecayBit"
            # Append to the end of DecayBit -- just before all_decays
            num = find_string("DecayBit.cpp", m, "void all_decays")[1]
            if output_opts.ch:
                amend_file("DecayBit.cpp", m, decaybit_src_ch, num-4, 
                           reset_contents)
                for i in xrange(len(decay_roll)):
                    if find_capability(decay_roll[i][0], m)[0]:
                        amend_rollcall(decay_roll[i][0], m, decay_roll[i][1], 
                                       reset_contents)
                    else:
                        num = find_string("DecayBit_rollcall.hpp", m, 
                                          "#define CAPABILITY decay_rates")[1]
                        amend_file("DecayBit_rollcall.hpp", m, decay_roll[i][1],
                                   num-2, reset_contents)
                if len(new_decays) > 0:
                    num = find_string("DecayBit_rollcall.hpp", m, 
                                      "MODEL_CONDITIONAL_DEPENDENCY"\
                                      "(MSSM_spectrum")[1]
                    amend_file("DecayBit_rollcall.hpp", m, all_decays_header, 
                               num-1, reset_contents)
                    num = find_string("DecayBit.cpp", m, "decays(\"omega\")")[1]
                    amend_file("DecayBit.cpp", m, all_decays_src, num, 
                               reset_contents)
            num = find_string("DecayBit.cpp", m, "void all_decays")[1]
            if output_opts.spheno:
                amend_file("DecayBit.cpp", m, spheno_decay_src, num-3, 
                            reset_contents)
                num = find_string("DecayBit_rollcall.hpp", m, 
                                  "#define CAPABILITY decay_rates")[1]
                amend_file("DecayBit_rollcall.hpp", m, spheno_decay_header,
                           num+2, reset_contents)

        # DarkBit
        if darkbit:
            m = "DarkBit"
            amend_rollcall("DarkMatter_ID", m, dmid_cap, reset_contents)
            amend_rollcall("DarkMatterConj_ID", m, dmconj_cap, reset_contents)
            write_file(gum.name + ".cpp", m, darkbit_src, reset_contents)
            if pc:
                amend_rollcall("TH_ProcessCatalog", m, pc_cap, reset_contents)
                add_new_model_to_function("DarkBit_rollcall.hpp", "DarkBit", 
                                          "RD_spectrum", 
                                          "RD_spectrum_from_ProcessCatalog",
                                          gum.name, reset_contents, 
                                          pattern="ALLOW_MODELS")
                add_new_model_to_function("DarkBit_rollcall.hpp", "DarkBit", 
                                          "RD_eff_annrate", 
                                          "RD_eff_annrate_from_ProcessCatalog",
                                          gum.name, reset_contents, 
                                          pattern="ALLOW_MODELS")

        # ColliderBit
        m = "ColliderBit"
        if output_opts.pythia:
            copy_file("models/"+gum.name+".hpp", m, output_dir, reset_contents,
                      existing = False)
            copy_file("models/"+gum.name+".cpp", m, output_dir, reset_contents, 
                      existing = False)

        # HiggsBounds interface
        if output_opts.spheno:
            if len(higgses) == 1:
                num = find_string("ColliderBit_Higgs.cpp", m, 
                        "ModelInUse(\"StandardModel_Higgs\")")[1]
                amend_file("ColliderBit_Higgs.cpp", m, hct_src, num-1, 
                           reset_contents)
                num = find_string("ColliderBit_Higgs_rollcall.hpp", m, 
                        "MODEL_CONDITIONAL_DEPENDENCY(ScalarSingletDM_Z2")[1]
                amend_file("ColliderBit_Higgs_rollcall.hpp", m, hct_head, num-1,
                           reset_contents)
            else:
                num = find_string("ColliderBit_Higgs.cpp", m, 
                        "No valid model for MSSMLikeHiggs_ModelParameters.")[1]
                amend_file("ColliderBit_Higgs.cpp", m, hct_src, num-1, 
                           reset_contents)
                num = find_string("ColliderBit_Higgs_rollcall.hpp", m, 
                        "MODEL_CONDITIONAL_DEPENDENCY(MSSM_spectrum")[1]
                amend_file("ColliderBit_Higgs_rollcall.hpp", m, hct_head, num-1, 
                           reset_contents)
            add_new_model_to_function("ColliderBit_Higgs_rollcall.hpp", 
                                      m, "HB_ModelParameters",
                                      hb_pattern, gum.name, reset_contents,
                                      pattern="ALLOW_MODELS")

        # Backends
        m = "Backends"
        rebuild_backends = []
        # cmake
        if output_opts.pythia:
            copy_file("backends.cmake", "cmake", output_dir, reset_contents, 
                      existing = True)
        # CalcHEP
        if output_opts.ch:
            f = "frontends/CalcHEP_3_6_27.cpp"
            num = find_string(f, m, "setModel(modeltoset, 1)")[1]
            amend_file(f, m, ch_src_sl, num-2, reset_contents)
            num = find_string(f, m, "END_BE_INI_FUNCTION")[1]
            amend_file(f, m, ch_src_pl, num-2, reset_contents)
            f = "frontends/CalcHEP_3_6_27.hpp"
            num = find_string(f, m, "backend_undefs.hpp")[1]
            amend_file(f, m, ch_head, num-2, reset_contents)
            num = find_string(f, m, "BE_FUNCTION")[1]
            amend_file(f, m, "BE_ALLOW_MODELS({0})".format(gum.name), num-2, 
                       reset_contents)
        # micrOMEGAs
        if output_opts.mo:
            print("Patching micrOMEGAs...")
            # Pristine and patched micrOMEGAs
            pristine_mo_dir = output_dir + "/micrOMEGAs"
            patched_mo_dir = output_dir + "/micrOMEGAs_patched"
            # Copy micromegas files to Backend patches from the cleaned
            # CalcHEP directory
            copy_micromegas_files(gum.name)
            # Add the patch file to the directory too
            patch_micromegas(gum.name, reset_contents)
            # Now write the headers
            ver = "3.6.9.2"
            be = "MicrOmegas_" + gum.name
            be_loc = "micromegas/{0}/{1}/libmicromegas.so".format(ver, gum.name)
            f = "frontends/MicrOmegas_{0}_{1}".format(gum.name, 
                                                      ver.replace('.','_'))
            write_file(f+".cpp", m, mo_src, reset_contents)
            write_file(f+".hpp", m, mo_head, reset_contents)
            add_to_backend_locations(be, be_loc, ver, reset_contents)
            add_micromegas_to_cmake(gum.name, reset_contents)
            add_micromegas_to_darkbit_rollcall(gum.name, reset_contents)

        # Pythia
        if output_opts.pythia:
            be = "pythia_"+gum.name.lower()
            safe_ver = "8_"+base_pythia_version
            ver = "8."+base_pythia_version
            copy_file("default_bossed_versions.hpp", m, output_dir, 
                      reset_contents, existing = True)
            copy_file("BOSS/configs/"+be+"_"+safe_ver+".py", m, output_dir, 
                      reset_contents, existing = False)
            copy_file(be+"/"+ver+"/patch_"+be+"_"+ver+".dif", m, output_dir, 
                      reset_contents, existing = False)
            add_to_backend_locations("Pythia_"+gum.name, 
                                     be+"/"+ver+"/lib/libpythia8.so", ver, 
                                     reset_contents)

        # SPheno
        if output_opts.spheno:
            ver = SPHENO_VERSION
            be = "SARAHSPheno_"+gum.name
            be_loc = (
                   "sarah-spheno/{0}/{1}/lib/libSPheno{2}.so"
            ).format(ver, gum.name, clean_model_name)
            f = "frontends/SARAHSPheno_{0}_{1}".format(gum.name,
                                                       ver.replace('.','_'))
            write_file(f+".cpp", m, spheno_src, reset_contents)
            write_file(f+".hpp", m, spheno_header, reset_contents)
            add_to_backend_locations(be, be_loc, ver, reset_contents)
            add_to_backends_cmake(spheno_cmake, reset_contents, 
                                  string_to_find="# gm2calc")
            # Move SPheno files to Backend/patches/...
            src = output_dir + "/SPheno/" + clean_model_name
            dst = (
                "../Backends/patches/sarah-spheno/{0}/{1}/unpatched"
            ).format(ver, gum.name)
            # Delete directory if it exists, then make it
            remove_tree_quietly(dst)
            mkdir_if_absent(dst)
            copy_tree(src, dst)
            patchloc = (
                     "sarah-spheno/{0}/{1}/patch_sarah-spheno_{0}_{1}.dif"
            ).format(ver, gum.name)
            copy_file(patchloc, m, output_dir, reset_contents, existing = False)
            # SPheno DecayTable
            filename = (
                "data/SARAHSPheno_{0}_{1}_decays_info.dat"
            ).format(gum.name, SPHENO_VERSION.replace('.','_'))
            write_file(filename, m, spheno_decay_tables, reset_contents)
            # SPheno backend_types 
            amend_file("backend_types/SPheno.hpp", m, backend_types, btnum-2, 
                       reset_contents)
            # Check if backend needs rebuild
            be_install_dir  = "../Backends/installed/sarah-spheno/" + ver + "/" + gum.name
            gum_patched_dir = output_dir + '/SPheno_patched'
            check_backend_rebuild('sarah-spheno_'+gum.name, ver, be_install_dir, 
                                  gum_patched_dir, rebuild_backends, 
                                  file_endings=('F90','f90','Makefile'), 
                                  build_dir='../build')

        # Vevacious
        if output_opts.vev:
            num = find_string("SpecBit_VS.cpp", "SpecBit",
                              "} // end namespace SpecBit")[1]
            amend_file("SpecBit_VS.cpp", "SpecBit", vev_src, num-1, 
                       reset_contents)
            write_vevacious_rollcall(gum.name, gum.spec, reset_contents)        

        # Write capability and model definitions
        write_capability_definitions("capabilities.dat", gum.name, capability_definitions, reset_contents)
        write_model_definitions("models.dat", gum.name, model_definitions, reset_contents)

        # Write a simple YAML file.
        drop_yaml_file(gum.name, model_parameters, add_higgs, reset_contents,
                       gum.spec, output_opts.spheno)
        print(("\nGUM has dropped a test YAML file at "
               "$GAMBIT/yaml_files/{0}_example.yaml!").format(gum.name))

        # Write a config file to make compilation easier
        write_config_file(output_opts, gum.name, reset_contents, rebuild_backends)
        print(("\nGUM has dropped a config script {0}_config.sh").format(gum.name))

        # Inform user to rebuild backends
        if rebuild_backends:
            print(("The following backends have been changed, if you are not using "
                   "the config script provided make sure to rebuild them: "
                   "{0}").format(rebuild_backends))

        # Save output to a .mug file for resetting purposes
        mug_file = "mug_files/{0}.mug".format(gum.name)
        drop_mug_file(mug_file, reset_contents)

        # Remove the temp file, don't need it now.
        os.remove("mug_files/temp.mug")

        print("")
        print("Changes saved to {}".format(mug_file))
        print("If you need to reset GAMBIT, do:")
        print("\t./gum -r {}".format(mug_file))

        # All done!
        print("")
        print("GUM has finished successfully!")
        print("Please (re)compile GAMBIT, by running:")
        print("\t bash {}_config.sh".format(gum.name))
        print("")

    except Exception as exc:
        print("\n\nGUM has failed!")
        if os.path.exists("mug_files/temp.mug"):
            print("Removing all added content...")
            blockPrint()                     # Stop the output for a sec
            revert("mug_files/temp.mug")     # Remove it all
            enablePrint()                    # Allow printing again
            os.remove("mug_files/temp.mug")  # Remove the temp mug file
            print("\nError message:\n")
        import traceback
        traceback.print_exc()


# If reset is called
elif args.reset:
    try:
        revert(args.reset)
        model = os.path.split(args.reset)[-1].strip('.mug')
        print("GUM has removed the model '{0}' from GAMBIT.".format(model))
    except Exception as exc:
        import traceback
        traceback.print_exc()

else:
    raise GumError(("\n\n\tHi! You must be new here. Usage: gum -f inifile.gum"
                    "\n\tOr try gum -h for help."))
